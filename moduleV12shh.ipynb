{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "TEs-taL3ZHtW",
    "outputId": "ba7fb690-6bb8-49ed-a8f2-911f58c4d27b",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1004\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1004\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error() {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < css_urls.length; i++) {\n",
       "      var url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\": \"T2yuo9Oe71Cz/I4X9Ac5+gpEa5a8PpJCDlqKYO0CfAuEszu1JrXLl8YugMqYe3sM\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\": \"98GDGJ0kOMCUMUePhksaQ/GYgB3+NH9h996V88sh3aOiUNX3N+fLXAtry6xctSZ6\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\": \"89bArO+nlbP3sgakeHjCo1JYxYR5wufVgA3IbUvDY+K7w4zyxJqssu7wVnfeKCq8\"};\n",
       "\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      if (url in hashes) {\n",
       "        element.crossOrigin = \"anonymous\";\n",
       "        element.integrity = \"sha384-\" + hashes[url];\n",
       "      }\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  \n",
       "  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\"];\n",
       "  var css_urls = [];\n",
       "  \n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "    \n",
       "    \n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "      \n",
       "    for (var i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "    if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1004\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1004\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\": \"T2yuo9Oe71Cz/I4X9Ac5+gpEa5a8PpJCDlqKYO0CfAuEszu1JrXLl8YugMqYe3sM\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\": \"98GDGJ0kOMCUMUePhksaQ/GYgB3+NH9h996V88sh3aOiUNX3N+fLXAtry6xctSZ6\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\": \"89bArO+nlbP3sgakeHjCo1JYxYR5wufVgA3IbUvDY+K7w4zyxJqssu7wVnfeKCq8\"};\n\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      if (url in hashes) {\n        element.crossOrigin = \"anonymous\";\n        element.integrity = \"sha384-\" + hashes[url];\n      }\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  \n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\"];\n  var css_urls = [];\n  \n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {\n    \n    \n    }\n  ];\n\n  function run_inline_js() {\n    \n    if (root.Bokeh !== undefined || force === true) {\n      \n    for (var i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n    if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1004\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from sys import exit\n",
    "from lxml import etree\n",
    "from scipy import signal\n",
    "from sklearn import preprocessing\n",
    "from scipy import optimize\n",
    "import regex as re\n",
    "import timeit\n",
    "from scipy.signal import butter, lfilter\n",
    "from scipy.signal import freqz\n",
    "from lmfit.models import StepModel, LinearModel\n",
    "import sys, importlib, os\n",
    "import McsPy.McsData\n",
    "import McsPy.McsCMOS\n",
    "from McsPy import ureg, Q_\n",
    "from numpy import trapz\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "import h5py\n",
    "import pickle\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import skew\n",
    "from scipy.stats import kurtosis\n",
    "from scipy.signal import find_peaks, peak_prominences\n",
    "%matplotlib inline\n",
    "import itertools\n",
    "from bokeh.io import output_notebook, show\n",
    "from bokeh.plotting import figure\n",
    "import ipyparams\n",
    "\n",
    "output_notebook()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function RawData.__del__ at 0x0000024E1831FE50>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\MEA_PC\\anaconda3\\lib\\site-packages\\McsPy\\McsData.py\", line 68, in __del__\n",
      "    if self.h5_file:\n",
      "AttributeError: 'RawData' object has no attribute 'h5_file'\n"
     ]
    }
   ],
   "source": [
    "def Fold_Folders(mainpath):\n",
    "    \n",
    "    \n",
    "    folder_path=mainpath+'\\\\'+'hdf5'\n",
    "    \n",
    "    metadatapath=mainpath+'\\\\'+'hdf5'\n",
    "    \n",
    "    save_to=mainpath+'\\\\'+'csv'\n",
    "    \n",
    "    \n",
    "    if 'csv' not in os.listdir(mainpath):\n",
    "        \n",
    "        os.mkdir(save_to)\n",
    "    \n",
    "    \n",
    "    \n",
    "    dirs=os.listdir(mainpath)\n",
    "    \n",
    "    walk=list(os.walk(mainpath))\n",
    "    \n",
    "    walkpaths=[i for index in walk  for i in index if isinstance(i, str)==True]\n",
    "    \n",
    "    if 'hdf5' in dirs:\n",
    "    \n",
    "        folder_path=mainpath+'\\\\'+'hdf5'\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        'Convert spikes and save in hdf5 folder'\n",
    "        \n",
    "    mwsdir=[i for i  in dirs if 'mws' in i ]\n",
    "    \n",
    "    \n",
    "    if len(mwsdir)>0:\n",
    "        \n",
    "        metadatapath=mainpath\n",
    "        \n",
    "    elif len(walkpaths)>0:\n",
    "        \n",
    "        for wpth in walkpaths:\n",
    "            \n",
    "            dirswpth=os.listdir(wpth)\n",
    "            \n",
    "            mwsdirpth=[i for i  in dirswpth if 'mws' in i ]\n",
    "            \n",
    "            if len(mwsdirpth)>0:\n",
    "                \n",
    "                metadatapath=wpth\n",
    "                \n",
    "    else:\n",
    "        print('no mwsin directory')\n",
    "        \n",
    "        \n",
    "        \n",
    "    return (folder_path, metadatapath, save_to)\n",
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "qKkl-JlAZHtb",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Raw Load function\n",
    "def load_raw(path, identifier, identifier2):\n",
    "\n",
    "    dirs = os.listdir(path)\n",
    "    \n",
    "    print(dirs, 'dirs')\n",
    "\n",
    "\n",
    "    filename=[]\n",
    "    experiments={}\n",
    "\n",
    "\n",
    "    for f in dirs:\n",
    "\n",
    "        if (re.search('h5', f) and re.search(identifier, f) and (identifier2 in f)):\n",
    "\n",
    "            filename.append(f)\n",
    "            MetadataFilename=path+'/'+f[:-7]+'.mws'\n",
    "\n",
    "            Recordingfile=path+'/'+f\n",
    "            recording=McsPy.McsData.RawData(Recordingfile)\n",
    "\n",
    "            tree = etree.parse(MetadataFilename)\n",
    "            ExperimentID=tree.xpath('//ExperimentID/text()')[0]\n",
    "            CompoundID=recording.comment\n",
    "            if ExperimentID not in experiments.keys():\n",
    "\n",
    "                experiments[ExperimentID]={CompoundID:[]}\n",
    "            else:\n",
    "                experiments[ExperimentID].update({CompoundID:[]})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return filename, experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "IcnsEiDXZHtc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def led_info(Recordingfilename, Metadatafile):\n",
    "\n",
    "    \"\"\"Fuction to extract the LED stimulated well label, led color, led times and duration of the pulse\"\"\"\n",
    "\n",
    "    R=McsPy.McsData.RawData(Recordingfilename)\n",
    "\n",
    "    Info={}\n",
    "\n",
    "    LED_T=[]\n",
    "    LED_D=[]\n",
    "    W=[]\n",
    "    L=[]\n",
    "\n",
    "\n",
    "    if len(list(R.recordings[0].event_streams.keys()))>3:\n",
    "\n",
    "\n",
    "\n",
    "        for i in list(R.recordings[0].event_streams[3].event_entity.keys()):\n",
    "\n",
    "\n",
    "\n",
    "            Wells=R.recordings[0].event_streams[3].event_entity[i].info.__dict__['info']['SourceChannelLabels']\n",
    "\n",
    "            Label=R.recordings[0].event_streams[3].event_entity[i].info.__dict__['info']['Label']\n",
    "\n",
    "            #print(R.recordings[0].event_streams[3].event_entity[i].info.__dict__['info']['EventID'], 'led')\n",
    "            LED_T.append(R.recordings[0].event_streams[3].event_entity[i].data[0].tolist())\n",
    "            LED_D.append(R.recordings[0].event_streams[3].event_entity[i].data[1].tolist())\n",
    "\n",
    "            W.append(Wells)\n",
    "            L.append(Label)\n",
    "\n",
    "\n",
    "\n",
    "        Info['Wells']=W\n",
    "        Info['Label']=L\n",
    "        Info['Time']=LED_T\n",
    "        Info['Duration']=LED_D\n",
    "\n",
    "\n",
    "    return Info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "iwsIs3ftZHte",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Dilutions(tree, Compound):\n",
    "    r=tree.getroot()\n",
    "    dilutions={}\n",
    "    for compound in r[1][0][1].getchildren():\n",
    "        Series=[]\n",
    "        for ser in compound[0][-1][0][1].getchildren():\n",
    "            Series.append(ser.attrib['Dilution'].split(\" \")[0])\n",
    "\n",
    "        dilutions[str(compound[0][1].text)]=Series\n",
    "\n",
    "    Labels=dilutions[Compound]\n",
    "\n",
    "    return Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "1Jn1KVywZHte",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def lab_book(tree):\n",
    "\n",
    "    labbook={}\n",
    "\n",
    "    root=tree.getroot()\n",
    "\n",
    "\n",
    "    lablist=root[1][1].getchildren()\n",
    "\n",
    "    for i in range(len(lablist)):\n",
    "\n",
    "\n",
    "        labbook[root[1][1][i].tag]=root[1][1][i].text\n",
    "\n",
    "\n",
    "    return labbook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(spks_wavef, ((mins*50)+(start*50)), amplitudes, X[:, 0], lof, \n",
    " #           np.concatenate((features, pca_features), axis=1), stdlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "6vWcKAOlZHtf",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sd_extension(mydict, i,  APs, ExperimentID, Welllabel, WellID, Channellabel, Compound, dose_label, es):\n",
    "\n",
    "    size=len(APs[1])\n",
    "\n",
    "    print(len(APs[1]),'times')\n",
    "\n",
    "    print(len(APs[2]),'amplitude')\n",
    "\n",
    "    print(len(APs[3]), 'duration')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    mydict['Timestamp [µs]'].extend(APs[1])\n",
    "    mydict['Peak Amplitudes'].extend(APs[2])\n",
    "    mydict['Duration'].extend(APs[3])\n",
    "    mydict['Experiment'].extend(np.repeat(ExperimentID, size))\n",
    "    mydict['Well Label'].extend(np.repeat(Welllabel[i], size))\n",
    "    mydict['Well ID'].extend(np.repeat(WellID[i], size))\n",
    "    mydict['Channel ID'].extend(np.repeat(i, size))\n",
    "    mydict['Channel Label'].extend(np.repeat(Channellabel[i], size))\n",
    "    mydict['Dose Label'].extend(np.repeat(dose_label, size))\n",
    "    mydict['ES_condition'].extend(np.repeat(es, size))\n",
    "    mydict['Compound ID'].extend(np.repeat(Compound, size))\n",
    "    mydict['Threshold'].extend(np.repeat(APs[6][0], size))\n",
    "    \n",
    "    mydict['BaselineM'].extend(np.repeat(APs[6][1], size))\n",
    "    \n",
    "   \n",
    " \n",
    "    return mydict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AP_detection_lofnewest(to, lof, signal, partlength, dead_time, threshold, negative, positive,\n",
    "                           fs, duration, learn, noisereduced, start=0):\n",
    "    \n",
    "    \n",
    "    ##PAYTSARS SUGGESTION; SPLIT INTO MANY PARTS TAKE MIN OF DITRIBUTION\n",
    "    split=int(len(signal)//(2*partlength))\n",
    "    \n",
    "    if split>0:\n",
    "        \n",
    "        parted_signal=np.array_split(signal, split)\n",
    "        \n",
    "        stds=[np.std(subsignal[:]) for subsignal in parted_signal]\n",
    "    else:\n",
    "        \n",
    "        stds=[np.std(signal[:])]\n",
    "        \n",
    "    \n",
    "    \n",
    "    dead_time=(fs/1000)*dead_time\n",
    "\n",
    "    duration=(fs/1000)*duration\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    pretrig=(fs/1000)*1\n",
    "    postrig=(fs/1000)*2\n",
    "\n",
    "    ##maximum spike duration\n",
    "    \n",
    "    maxdur=duration*1.5\n",
    "    \n",
    "\n",
    "\n",
    "    ##thresholds\n",
    "    #stds=[np.std(subsignal[:]) for subsignal in parted_signal]\n",
    "    \n",
    "    stdused=np.std(signal[:])\n",
    "    \n",
    "    minstd=min(stds)\n",
    "    meanstd=np.mean(stds)\n",
    "    \n",
    "    medianstd=np.median(stds)\n",
    "    \n",
    "    stdlist=[minstd, medianstd]\n",
    "    \n",
    "    if positive==False:\n",
    "        \n",
    "        negpeak_idxs, pospeak_idxs=peaknew(signal, positive, negative)\n",
    "        \n",
    "        \n",
    "        \n",
    "        threshold_crossings_idx=np.where(signal <(-threshold*medianstd))[0]\n",
    "        \n",
    "        #print(negpeak_idxs, pospeak_idxs, threshold_crossings_idx)\n",
    "        \n",
    "        peak_idx=np.intersect1d(negpeak_idxs, threshold_crossings_idx)\n",
    "\n",
    "    else:\n",
    "        negpeak_idxs, pospeak_idxs=peaknew(signal, positive, negative)\n",
    "        \n",
    "        threshold_crossings_idx=np.where((signal>(threshold)*medianstd) | (signal<(-threshold)*medianstd))[0]\n",
    "        \n",
    "        #print(negpeak_idxs[:10], pospeak_idxs[:10], threshold_crossings_idx[:10])\n",
    "        \n",
    "        peak_idx=negpeak_idxs+pospeak_idxs\n",
    "        \n",
    "        \n",
    "    \n",
    "        peak_idx=np.intersect1d(peak_idx, threshold_crossings_idx)\n",
    "\n",
    "\n",
    "\n",
    "    ###should not start with spike\n",
    "    indexes=[i for i in range(len(peak_idx)) if (((peak_idx[i]-int(duration))>0) & ((peak_idx[i]+duration)<len(signal)))]\n",
    "    mins=np.array(sorted(peak_idx[indexes]))\n",
    "    \n",
    "    \n",
    "    \"\"\"removing spikes that violate dead time criteria also 50us peaks\"\"\"\n",
    "\n",
    "    while np.any(np.diff(mins)<dead_time):\n",
    "\n",
    "        dead_idx=(np.where(np.diff(mins)<dead_time)[0]+1).tolist()\n",
    "        mins=np.delete(mins, dead_idx)\n",
    "        \n",
    "        \n",
    "    if noisereduced==True:\n",
    "        \n",
    "        noisikner=PipNoise_detection(signal, 0.5, 5, True, True, 20000, 3)[1]\n",
    "        \n",
    "        print(noisikner, 'noisikner', len(noisikner))\n",
    "        \n",
    "        \n",
    "        if len(noisikner)>0:\n",
    "            \n",
    "            #print(noisikner, 'sectime')\n",
    "            \n",
    "        \n",
    "            noisik=np.array([np.arange(n-500, n+500, 1) for n in noisikner]).ravel()\n",
    "            \n",
    "            #print(noisik, 'noisik')\n",
    "            mins=np.array([m for m in mins if m not in noisik ])\n",
    "            \n",
    "            print(mins, 'after noisik ')\n",
    "            \n",
    "    if len(mins)>1:\n",
    "\n",
    "\n",
    "        # for testing will be >=1 but for the data >1\n",
    "\n",
    "\n",
    "\n",
    "        spks_wavef=np.array([signal[i-int(duration/2):i+int(duration/2)+1] for i in mins])\n",
    "        \n",
    "        print(spks_wavef.shape, duration, 'shapedur', len(mins))\n",
    "\n",
    "\n",
    "        amplitudes=np.median(spks_wavef[:, :], axis=1)-spks_wavef[:, int(spks_wavef.shape[1]/2)]\n",
    "\n",
    "        features=np.apply_along_axis(wvf_features, 1, spks_wavef)\n",
    "        \n",
    "        #amplitudes=np.apply_along_axis(peak_prominences, 1, spks_wavef, [int(spks_wavef.shape[1]/2)] )[:, 0]\n",
    "        \n",
    "        #print(amplitudes.shape, 'amplshape',  int(spks_wavef.shape[1]/2))\n",
    "        \n",
    "        pca_features=pca(spks_wavef, 4)\n",
    "        \n",
    "        X=np.concatenate((amplitudes.reshape(amplitudes.shape[0], 1), pca_features), axis=1)\n",
    "        \n",
    "        print(X.shape,  spks_wavef.shape, 'shapes')\n",
    "\n",
    "\n",
    "\n",
    "        if len(mins)>500 and learn==True:\n",
    "            lof.fit(X)\n",
    " \n",
    "       \n",
    "\n",
    "    else:\n",
    "        X=np.empty([0, 5])\n",
    "        mins=np.array([])\n",
    "        spks_wavef=np.empty([0, int(duration+1)])\n",
    "        amplitudes=np.array([])\n",
    "        features=np.empty([0, 24])\n",
    "        pca_features=np.empty([0, 4])\n",
    "        stdlist=[[], []]\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(signal)\n",
    "    \n",
    "    if len(mins)>0:\n",
    "\n",
    "\n",
    "        y = np.interp(mins, np.arange(0, len(signal), 1), signal)\n",
    "        ax.plot(mins, y, ls=\"\", marker=\"*\", ms=15,  color=\"crimson\", alpha=0.3)\n",
    "        plt.axhline(y=(-threshold*medianstd))\n",
    "        plt.axhline(y=(threshold*medianstd))\n",
    "    #plt.savefig(to+'/'+str(len(mins))+'Spikes'+'.tif', format='tif')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    #print(features.shape,'rfshape' )\n",
    "\n",
    "    #print(pca_features.shape,'pcarfshape' )\n",
    "    \n",
    "    \n",
    "    #print(spks_wavef.shape, len(((mins*50)+(start*50))),  len(amplitudes), np.concatenate((features, pca_features), axis=1).shape)\n",
    "    #print(mins, 'minslater')\n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    return (spks_wavef, ((mins*50)+(start*50)), amplitudes, X[:, 0], lof, \n",
    "            np.concatenate((features, pca_features), axis=1), stdlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "bJHmj7PnZHti",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def peak(cut, positive, negative):\n",
    "\n",
    "    sign_diff=np.diff(np.sign(np.diff(cut)))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    ###print(cut,  np.diff(cut),  np.sign(np.diff(cut)), sign_diff, 'signdiffpeak')\n",
    "\n",
    "    peak_idx=[]\n",
    "\n",
    "\n",
    "    if negative==True:\n",
    "\n",
    "        peak_indices=(np.where(sign_diff==2)[0]+1).tolist()\n",
    "        if len(peak_indices)>0:\n",
    "            #all negative peaks  peaks\n",
    "            pk=np.argmin(cut[peak_indices])\n",
    "            peak_idx=[peak_indices[pk]]\n",
    "\n",
    "    if (positive==True) and (len(peak_idx)==0):\n",
    "\n",
    "        peak_indices=(np.where(sign_diff==-2)[0]+1).tolist()\n",
    "        if len(peak_indices)>0:\n",
    "            #all negative peaks  peaks\n",
    "            pk=np.argmax(cut[peak_indices])\n",
    "            peak_idx=[peak_indices[pk]]\n",
    "\n",
    "        #maximum negative peak actual index\n",
    "    #np.where((sign_diff==2)|(sign_diff==-2))[0].tolist()\n",
    "    #So, really it would be nice to have both positive and negative peaks' cutouts and find AP via negative peak.\n",
    "\n",
    "\n",
    "    return (peak_idx, peak_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "bJHmj7PnZHti",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def peaknew(cut, positive, negative):\n",
    "\n",
    "    sign_diff=np.diff(np.sign(np.diff(cut)))\n",
    "    \n",
    "\n",
    "    ###print(cut,  np.diff(cut),  np.sign(np.diff(cut)), sign_diff, 'signdiffpeak')\n",
    "\n",
    "    positive_index=[]\n",
    "    \n",
    "    \n",
    "    negative_index=[]\n",
    "\n",
    "\n",
    "    if negative==True:\n",
    "\n",
    "        negative_index=(np.where(sign_diff==2)[0]+1).tolist()\n",
    "       \n",
    "\n",
    "    if (positive==True):\n",
    "\n",
    "        positive_index=(np.where(sign_diff==-2)[0]+1).tolist()\n",
    "        #maximum negative peak actual index\n",
    "    #np.where((sign_diff==2)|(sign_diff==-2))[0].tolist()\n",
    "    #So, really it would be nice to have both positive and negative peaks' cutouts and find AP via negative peak.\n",
    "\n",
    "\n",
    "    return (negative_index, positive_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "bJHmj7PnZHti",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def peakslope(cut, positive, negative):\n",
    "\n",
    "    sign_diff=np.diff(np.sign(np.diff(cut)))\n",
    "    \n",
    "\n",
    "    ###print(cut,  np.diff(cut),  np.sign(np.diff(cut)), sign_diff, 'signdiffpeak')\n",
    "\n",
    "    positive_index=[]\n",
    "    \n",
    "    \n",
    "    negative_index=[]\n",
    "\n",
    "\n",
    "    if negative==True:\n",
    "\n",
    "        negative_index=(np.where(sign_diff==2)[0]+1).tolist()\n",
    "       \n",
    "\n",
    "    if (positive==True):\n",
    "\n",
    "        positive_index=(np.where(sign_diff==-2)[0]+1).tolist()\n",
    "        #maximum negative peak actual index\n",
    "    #np.where((sign_diff==2)|(sign_diff==-2))[0].tolist()\n",
    "    #So, really it would be nice to have both positive and negative peaks' cutouts and find AP via negative peak.\n",
    "\n",
    "\n",
    "    return (negative_index, positive_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "C4mazZGXZHtj",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def wvf_features(wvf):\n",
    "\n",
    "    features=np.zeros(24)\n",
    "\n",
    "    #shape\n",
    "\n",
    "\n",
    "    FD=np.diff(wvf)\n",
    "    sign_diff=np.diff(np.sign(FD))\n",
    "    zero_cross=(np.where((sign_diff==-2)|(sign_diff==2))[0])\n",
    "    #zer_cross=(np.where((sign_diff==2))[0]+1)\n",
    "    ##print('zero cross', zer_cross)\n",
    "\n",
    "    AP_peak_cross=int(len(wvf)//2)-1 #AP peak index in FD (zero_crossing)\n",
    "\n",
    "\n",
    "    try:\n",
    "        peaks=np.where(np.diff(np.sign(np.diff(FD)))==-2)[0]+1\n",
    "    except:\n",
    "        peaks=np.array([])\n",
    "    try:\n",
    "        valleys=np.where(np.diff(np.sign(np.diff(FD)))==2)[0]+1\n",
    "    except:\n",
    "        valleys=np.array([])\n",
    "    try:\n",
    "        p1=np.sort(peaks[(peaks-AP_peak_cross)<0])[-1]\n",
    "    except:\n",
    "        p1=0\n",
    "    try:\n",
    "        p2=np.argmin(FD)\n",
    "    except:\n",
    "        p2=0\n",
    "    ### valley of FD\n",
    "    try:\n",
    "        p3=AP_peak_cross\n",
    "    except:\n",
    "        p3=0\n",
    "        ## second zero crossing (crossing )\n",
    "    try:\n",
    "        p4=np.sort(peaks[(peaks-AP_peak_cross)>0])[0]\n",
    "    except:\n",
    "        p4=0\n",
    "    try:#np.argmax(FD) #peak of PD\n",
    "        p5=np.sort(zero_cross[(zero_cross-AP_peak_cross)>0])[0]\n",
    "    except:\n",
    "        p5=0#zero_crossing after AP_zero_cross\n",
    "    try:\n",
    "        p6=np.sort(valleys[(valleys-AP_peak_cross)>0])[0]\n",
    "    except:\n",
    "        p6=0#valley after zero cross\n",
    "\n",
    "\n",
    "    #fig, ax = plt.subplots()\n",
    "    #ax.plot(FD)\n",
    "\n",
    "    #y = np.interp([p1, p2, p3, p4, p5, p6], np.arange(0, len(FD), 1), FD)\n",
    "    #ax.plot([p1, p2, p3, p4, p5, p6], y, ls=\"\", marker=\"*\", ms=15,  color=\"crimson\")\n",
    "    #plt.title('Feature guides')\n",
    "    #plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    try:\n",
    "        features[0]=p5-p1\n",
    "    except:\n",
    "        features[0]=0\n",
    "\n",
    "    try:\n",
    "        features[1]= FD[p4]-FD[p2]\n",
    "    except:\n",
    "\n",
    "        features[1]=0\n",
    "\n",
    "    try:\n",
    "        features[2]=FD[p6]-FD[p2]  #F3 correlation with reference waveform is missing\n",
    "    except:\n",
    "        features[2]=0\n",
    "\n",
    "    try:\n",
    "        features[3]=np.log(np.abs((FD[p4]-FD[p2])/(p4-p2)))\n",
    "\n",
    "    except:\n",
    "        features[3]=0\n",
    "\n",
    "    try:\n",
    "        features[4]=(FD[p6]-FD[p4])/(p6-p4)\n",
    "\n",
    "    except:\n",
    "        features[4]=0\n",
    "\n",
    "    try:\n",
    "        features[5]=np.log(np.abs((FD[p6]-FD[p2])/(p6-p2)))\n",
    "\n",
    "    except:\n",
    "        features[5]=0\n",
    "\n",
    "    try:\n",
    "        features[6]=np.sqrt(np.abs(np.mean(FD[:p2]))) #root mean square of pre-event amplitudes (added np/abs to avoid neg.)\n",
    "\n",
    "    except:\n",
    "        features[6]=0\n",
    "    try:\n",
    "        features[7]=((FD[p2]-FD[p1])/(p2-p1))/((FD[p3]-FD[p2])/(p3-p2))\n",
    "\n",
    "    except:\n",
    "        features[7]=0\n",
    "\n",
    "    try:\n",
    "        features[8]=((FD[p4]-FD[p3])/(p4-p3))/((FD[p5]-FD[p4])/(p5-p4))\n",
    "\n",
    "    except:\n",
    "        features[8]=0\n",
    "\n",
    "    try:\n",
    "        features[9]=FD[p2]/FD[p4]\n",
    "\n",
    "    except:\n",
    "        features[9]=0\n",
    "\n",
    "    #phase based features\n",
    "\n",
    "    features[10:16]=FD[[p1, p2, p3, p4, p5, p6]]\n",
    "    try:\n",
    "        features[16:19]=np.diff(FD)[[p1-1, p3-1, p5-1]] #second derivative of  zeros_crossings\n",
    "    except:\n",
    "        features[16:19]=0\n",
    "\n",
    "\n",
    "    #distribution based features\n",
    "\n",
    "    features[19]=np.percentile(FD, 75)-np.percentile(FD, 25)\n",
    "    features[20]=np.percentile(np.diff(FD), 75)-np.percentile(np.diff(FD), 25)\n",
    "    features[21]=skew(FD)\n",
    "    features[22]=skew(np.diff(FD))\n",
    "    features[23]=kurtosis(FD)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "7G5w0FmZZHtk",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def pca(wvfs, comps):\n",
    "\n",
    "    if wvfs.shape[0]>10:\n",
    "\n",
    "\n",
    "        pca=PCA(n_components=comps)\n",
    "        wvfs_pca=pca.fit_transform(wvfs)\n",
    "    else:\n",
    "        wvfs_pca=np.zeros([wvfs.shape[0], 4])\n",
    "\n",
    "\n",
    "\n",
    "    return wvfs_pca\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "C6BD9UodZHtk",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Clusterization\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eh17ylNjZHtk",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "DLG2vBEMZHtk",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def AP_cutouts(v, dead_time, threshold, negative, positive, fs, duration, start=0):\n",
    "\n",
    "    \"\"\"The spike detection function, which takes channel\n",
    "    raw data as an input and outputs spike parameters, here the spike cutout also beign extracted, amplitude is absolute\"\"\"\n",
    "\n",
    "\n",
    "    dv=np.diff(v)#the derivative of the signal\n",
    "\n",
    "    sign_diff=np.diff(np.sign(dv)) #the difference of the signs of derivative\n",
    "\n",
    "     #parameter to  filter spikes by duration\n",
    "\n",
    "    dead_time=(dead_time*fs)/1000  #parameter for refractory period consideration\n",
    "\n",
    "\n",
    "    if negative==False:  #detect positive peaks\n",
    "\n",
    "\n",
    "        peak_idx=(np.where(sign_diff==(-2))[0]+1).tolist() # idx where the sign difference is -2, idx+1 is peak\n",
    "        threshold_crossings_idx=np.where(v>(threshold)*np.std(v[:]))[0] #threshold filtering, via std of 1s of data\n",
    "        AP_idx=np.intersect1d(peak_idx, threshold_crossings_idx)# intersect peaks and thresholds\n",
    "\n",
    "    if positive==False:   #detect negative peaks\n",
    "\n",
    "        peak_idx=(np.where(sign_diff==(2))[0]+1).tolist()#\n",
    "        threshold_crossings_idx=np.where(v<(-threshold)*np.std(v[:]))[0]\n",
    "        AP_idx=np.intersect1d(peak_idx, threshold_crossings_idx)\n",
    "\n",
    "    if (positive==True) and (negative==True): #detect both, positive and negative peaks\n",
    "\n",
    "        peak_idx=(np.where((sign_diff==(-2)) | (sign_diff==(2)))[0]+1).tolist()\n",
    "\n",
    "        threshold_crossings_idx=np.where((v>(threshold)*np.std(v[:])) | (v<(-threshold)*np.std(v[:])))[0]\n",
    "\n",
    "        AP_idx=np.intersect1d(peak_idx, threshold_crossings_idx)\n",
    "\n",
    "        dead_idx=(np.where(np.diff(AP_idx)<dead_time)[0]+1).tolist()\n",
    "        AP_idx=np.delete(AP_idx, dead_idx)\n",
    "\n",
    "    while np.any(np.diff(AP_idx)<dead_time):\n",
    "\n",
    "        dead_idx=(np.where(np.diff(AP_idx)<dead_time)[0]+1).tolist()\n",
    "        AP_idx=np.delete(AP_idx, dead_idx)\n",
    "\n",
    "\n",
    "\n",
    "    spks_wavef=np.array([v[i-int(duration/2):i+int(duration/2)] for i in AP_idx])\n",
    "\n",
    "    features=np.apply_along_axis(wvf_features, 1, spks_wavef)\n",
    "    pca_features=pca(spks_wavef, 4)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(signal)\n",
    "\n",
    "\n",
    "    y = np.interp(AP_idx, np.arange(0, len(v), 1), v)\n",
    "    ax.plot(AP_idx, y, ls=\"\", marker=\"*\", ms=15,  color=\"crimson\")\n",
    "    plt.axhline(y=(-threshold*np.std(v[:])))\n",
    "    plt.savefig(to+'/'+str(len(AP_idx))+'Spikes'+'.tif', format='tif')\n",
    "    plt.show()\n",
    "\n",
    "    return (AP_idx[1:], ((np.array(AP_idx)*50)+(start*50))[1:].tolist(),  features[1:, :], pca_features[1:, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thresholdsplot(signal, threshold):\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    plt.plot(stds)\n",
    "    plt.hlines(y=stdused, xmin=0, xmax=180, colors='red', label='Before')\n",
    "    plt.hlines(y=meanstd, xmin=0, xmax=180, colors='black', label='Mean', linestyles='dotted')\n",
    "    plt.hlines(y=minstd, xmin=0, xmax=180, colors='green', label='Min')\n",
    "    plt.hlines(y=medianstd, xmin=0, xmax=180, colors='yellow', label='Median')\n",
    "    \n",
    "    plt.legend()\n",
    "    \n",
    "    plt.title('stdsvalues')\n",
    "   \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "tP_EoEASZHtl",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def AP_detection_lofnew(to, lof, signal, dead_time, threshold, negative, positive, fs, duration, learn, start=0):\n",
    "    \n",
    "    \n",
    "    ##PAYTSARS SUGGESTION; SPLIT INTO MANY PARTS TAKE MIN OF DITRIBUTION\n",
    "    split=int(len(signal)//(2*20000))\n",
    "    parted_signal=np.array_split(signal, split)\n",
    "    \n",
    "    \n",
    "    dead_time=(fs/1000)*dead_time\n",
    "\n",
    "    duration=(fs/1000)*duration\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    pretrig=(fs/1000)*1\n",
    "    postrig=(fs/1000)*2\n",
    "\n",
    "    ##maximum spike duration\n",
    "    \n",
    "    maxdur=duration*1.5\n",
    "    \n",
    "\n",
    "\n",
    "    ##thresholds\n",
    "    stds= [np.std(subsignal[:]) for subsignal in parted_signal]\n",
    "    \n",
    "    stdused=np.std(signal[:])\n",
    "    \n",
    "    minstd=min(stds)\n",
    "    meanstd=np.mean(stds)\n",
    "    \n",
    "    medianstd=np.median(stds)\n",
    "    \n",
    "    stdlist=[minstd, medianstd]\n",
    "    \n",
    "    if positive==False:\n",
    "        \n",
    "        negpeak_idxs, pospeak_idxs=peaknew(signal, positive, negative)\n",
    "        \n",
    "        \n",
    "        \n",
    "        threshold_crossings_idx=np.where(signal <(-threshold*medianstd))[0]\n",
    "        \n",
    "        #print(negpeak_idxs, pospeak_idxs, threshold_crossings_idx)\n",
    "        \n",
    "        peak_idx=np.intersect1d(negpeak_idxs, threshold_crossings_idx)\n",
    "\n",
    "    else:\n",
    "        negpeak_idxs, pospeak_idxs=peaknew(signal, positive, negative)\n",
    "        \n",
    "        threshold_crossings_idx=np.where((signal>(threshold)*medianstd) | (signal<(-threshold)*medianstd))[0]\n",
    "        \n",
    "        #print(negpeak_idxs[:10], pospeak_idxs[:10], threshold_crossings_idx[:10])\n",
    "        \n",
    "        peak_idx=negpeak_idxs+pospeak_idxs\n",
    "        \n",
    "        \n",
    "    \n",
    "        peak_idx=np.intersect1d(peak_idx, threshold_crossings_idx)\n",
    "\n",
    "\n",
    "\n",
    "    ###should not start with spike\n",
    "    indexes=[i for i in range(len(peak_idx)) if (((peak_idx[i]-int(duration))>0) & ((peak_idx[i]+duration)<len(signal)))]\n",
    "    mins=np.array(sorted(peak_idx[indexes]))\n",
    "    \n",
    "    \n",
    "    \"\"\"removing spikes that violate dead time criteria also 50us peaks\"\"\"\n",
    "\n",
    "    while np.any(np.diff(mins)<dead_time):\n",
    "\n",
    "        dead_idx=(np.where(np.diff(mins)<dead_time)[0]+1).tolist()\n",
    "        mins=np.delete(mins, dead_idx)\n",
    "   \n",
    "\n",
    "   \n",
    "    if len(mins)>1:\n",
    "\n",
    "\n",
    "        # for testing will be >=1 but for the data >1\n",
    "\n",
    "\n",
    "\n",
    "        spks_wavef=np.array([signal[i-int(duration/2):i+int(duration/2)+1] for i in mins])\n",
    "        \n",
    "        print(spks_wavef.shape, duration, 'shapedur', len(mins))\n",
    "\n",
    "\n",
    "        amplitudes=np.median(spks_wavef[:, :], axis=1)-spks_wavef[:, int(spks_wavef.shape[1]/2)]\n",
    "\n",
    "        features=np.apply_along_axis(wvf_features, 1, spks_wavef)\n",
    "        \n",
    "        #amplitudes=np.apply_along_axis(peak_prominences, 1, spks_wavef, [int(spks_wavef.shape[1]/2)] )[:, 0]\n",
    "        \n",
    "        #print(amplitudes.shape, 'amplshape',  int(spks_wavef.shape[1]/2))\n",
    "        \n",
    "        pca_features=pca(spks_wavef, 4)\n",
    "        \n",
    "        X=np.concatenate((amplitudes.reshape(amplitudes.shape[0], 1), pca_features), axis=1)\n",
    "        \n",
    "        print(X.shape,  spks_wavef.shape, 'shapes')\n",
    "\n",
    "\n",
    "\n",
    "        if len(mins)>500 and learn==True:\n",
    "            lof.fit(X)\n",
    " \n",
    "       \n",
    "\n",
    "    else:\n",
    "        X=np.empty([0, 5])\n",
    "        mins=np.array([])\n",
    "        spks_wavef=np.empty([0, int(duration+1)])\n",
    "        amplitudes=np.array([])\n",
    "        features=np.empty([0, 24])\n",
    "        pca_features=np.empty([0, 4])\n",
    "        stdlist=[[], []]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(signal)\n",
    "\n",
    "\n",
    "    y = np.interp(mins, np.arange(0, len(signal), 1), signal)\n",
    "    ax.plot(mins, y, ls=\"\", marker=\"*\", ms=15,  color=\"crimson\", alpha=0.3)\n",
    "    plt.axhline(y=(-threshold*medianstd))\n",
    "    plt.axhline(y=(threshold*medianstd))\n",
    "    #plt.savefig(to+'/'+str(len(mins))+'Spikes'+'.tif', format='tif')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    #print(features.shape,'rfshape' )\n",
    "\n",
    "    #print(pca_features.shape,'pcarfshape' )\n",
    "    \n",
    "    \n",
    "    print(spks_wavef.shape, len(((mins*50)+(start*50))),  len(amplitudes), np.concatenate((features, pca_features), axis=1).shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return (spks_wavef, ((mins*50)+(start*50)), amplitudes, X[:, 0], lof, \n",
    "            np.concatenate((features, pca_features), axis=1), stdlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "gUoC1qK9ZHtl",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def AP_detection_lof(to, lof, signal, dead_time, threshold, negative, positive, fs, duration, learn, start=0):\n",
    "\n",
    "    if positive==False:\n",
    "        r1=(signal <=(-threshold*np.std(signal[10000:]))).astype('int64')\n",
    "\n",
    "    else:\n",
    "         r1=((signal <=(-threshold*np.std(signal[:]))) | (signal >=(threshold*np.std(signal[:])))).astype('int64')\n",
    "\n",
    "\n",
    "    dead_time=(fs/1000)*dead_time\n",
    "\n",
    "    duration=(fs/1000)*duration\n",
    "\n",
    "    buffer=20 #1ms to detect aps with positive threshold crossings but not negative\n",
    "    \n",
    "    \n",
    "    r1=r1[np.where(r1==0)[0][0]:]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    diffr1=np.diff(r1)\n",
    "    ###print(r1[:5000])\n",
    "    rights=np.where(diffr1==(-1))[0]+1\n",
    "    lefts=np.where(diffr1==1)[0]\n",
    "\n",
    "    #ranges=np.where(diffr1==(-1))[0]-np.where(diffr1==1)[0]\n",
    "    lefts=lefts[:len(rights)]\n",
    "    rights=rights[:len(lefts)] #\n",
    "\n",
    "\n",
    "    #in range min index + left original index\n",
    "\n",
    "    #dead time_sufficient\n",
    "    dead_idx=(np.where(np.diff(lefts)<dead_time)[0]+1).tolist()\n",
    "    lefts=np.delete(lefts, dead_idx)\n",
    "    rights=np.delete(rights, dead_idx)\n",
    "    ###print(len(lefts))\n",
    "\n",
    "    \"\"\"removing spikes that violate dead time criteria also 50us peaks\"\"\"\n",
    "\n",
    "    while np.any(np.diff(lefts)<dead_time) or np.any((rights-lefts)<1) or np.any((rights-lefts)>60):\n",
    "\n",
    "        dead_idx=(np.where(np.diff(lefts)<dead_time)[0]+1).tolist()\n",
    "        lefts=np.delete(lefts, dead_idx)\n",
    "        rights=np.delete(rights, dead_idx)\n",
    "    #no data\n",
    "        to_delmin=np.where((rights-lefts)<1)[0].tolist()\n",
    "        lefts=np.delete(lefts, to_delmin)\n",
    "        rights=np.delete(rights, to_delmin)\n",
    "\n",
    "        to_delmax=np.where((rights-lefts)>60)[0].tolist()\n",
    "        lefts=np.delete(lefts, to_delmax)\n",
    "        rights=np.delete(rights, to_delmax)\n",
    "\n",
    "    #try:\n",
    "\n",
    "    ##print(lefts, 'lefts')\n",
    "    ##print(rights, 'rights')\n",
    "\n",
    "    mins=np.array([peak(signal[(lefts[i]):(rights[i]+1)], positive, negative)[0][0]+(lefts[i]) for i in range(len(lefts))\n",
    "                   if len(peak(signal[(lefts[i]):(rights[i]+1)], positive, negative)[0])>0]) #future peak search\n",
    "    ind=[i for i in range(len(lefts)) if len(peak(signal[lefts[i]:(rights[i]+1)], positive, negative)[0])>0]\n",
    "    lefts=lefts[ind]\n",
    "    rights=rights[ind]#future peak search\n",
    "\n",
    "    indexes=[i for i in range(len(mins)) if (((mins[i]-int(duration/2))>0) & ((mins[i]+int(duration/2))<len(signal)))]\n",
    "    mins=mins[indexes]\n",
    "    lefts=lefts[indexes]\n",
    "    rights=rights[indexes]\n",
    "\n",
    "\n",
    "\n",
    "    #except:\n",
    "        #ValueError\n",
    "\n",
    "\n",
    "\n",
    "        #mins=[]\n",
    "\n",
    "\n",
    "    if len(mins)>1:\n",
    "\n",
    "        divide=(mins-lefts)\n",
    "        divide[divide==0]=1\n",
    "        rise_vel=(np.abs(signal[mins]-signal[lefts]))/divide\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        divide=(rights-mins)\n",
    "        divide[divide==0]=1\n",
    "        fall_vel=(np.abs(signal[rights]-signal[mins]))/divide\n",
    "\n",
    "        # for testing will be >=1 but for the data >1\n",
    "\n",
    "\n",
    "\n",
    "        spks_wavef=np.array([signal[i-int(duration/2):i+int(duration/2)] for i in mins])\n",
    "\n",
    "\n",
    "        amplitudes=spks_wavef[:, 0]-spks_wavef[:, int(spks_wavef.shape[1]/2)]\n",
    "\n",
    "        features=np.apply_along_axis(wvf_features, 1, spks_wavef)\n",
    "\n",
    "\n",
    "\n",
    "        pca_features=pca(spks_wavef, 4)\n",
    "\n",
    "\n",
    "\n",
    "        X=np.concatenate((rise_vel.reshape(rise_vel.shape[0], 1), fall_vel.reshape(rise_vel.shape[0], 1),\n",
    "                          amplitudes.reshape(amplitudes.shape[0], 1)), axis=1)\n",
    "        #strange_index=np.where(fall_vel>10)[0].tolist()\n",
    "        #strange=mins[strange_index]\n",
    "        ###print(mins)\n",
    "\n",
    "\n",
    "\n",
    "        if len(mins)>500 and learn==True:\n",
    "            lof.fit(X)\n",
    "  #u  outliers\n",
    "        ###print(mins)\n",
    "\n",
    "        if len(mins)>0:\n",
    "\n",
    "            plt.figure()\n",
    "            plt.xlabel('rise_vel')\n",
    "            plt.ylabel('Fall_vel')\n",
    "            plt.scatter(rise_vel, fall_vel)\n",
    "            plt.scatter(X[:, 0], X[:, 1], color='yellow', alpha=0.2)\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "    else:\n",
    "        X=np.empty([0, 2])\n",
    "        mins=np.array([])\n",
    "        spks_wavef=np.empty([0, 60])\n",
    "        amplitudes=np.array([])\n",
    "        features=np.empty([0, 24])\n",
    "        pca_features=np.empty([0, 4])\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(signal)\n",
    "\n",
    "\n",
    "    y = np.interp(mins, np.arange(0, len(signal), 1), signal)\n",
    "    ax.plot(mins, y, ls=\"\", marker=\"*\", ms=15,  color=\"crimson\")\n",
    "    plt.axhline(y=(-threshold*np.std(signal[:])))\n",
    "    #plt.savefig(to+'/'+str(len(mins))+'Spikes'+'.tif', format='tif')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    #plt.figure()\n",
    "    #plt.plot(spks_wavef[0, :])\n",
    "    #plt.show()\n",
    "\n",
    "    #plt.figure()\n",
    "    #plt.hist(rise_vel, bins='auto')\n",
    "    #plt.show()\n",
    "\n",
    "    #plt.figure()\n",
    "    #plt.xlabel('rise_vel')\n",
    "    #plt.ylabel('Fall_vel')\n",
    "    #plt.scatter(rise_vel, fall_vel)\n",
    "    #plt.scatter(X[:, 0], X[:, 1], color='yellow', alpha=0.2)\n",
    "\n",
    "    #plt.savefig(to+'/'+str(len(mins))+'Features'+'.tif', format='tif')\n",
    "\n",
    "    #only returns rise_vel but fall _vell is also important\n",
    "    #Future spike wvf feature extraction , classification\n",
    "    ##print(features.shape,'rfshape' )\n",
    "\n",
    "    ##print(pca_features.shape,'pcarfshape' )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return (spks_wavef, ((mins*50)+(start*50)), amplitudes, X[:, 0], lof, np.concatenate((features, pca_features), axis=1), threshold*np.std(signal[:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "uEoEQhw8ZHtm",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def AP_detection_lofNoise(signal, baseline, dead_time, threshold, negative, positive, fs, duration, start=0):\n",
    "\n",
    "    if positive==False:\n",
    "        r1=(signal <=(-threshold*np.std(signal[:]))).astype('int64')\n",
    "\n",
    "    else:\n",
    "         r1=((signal <=(-threshold*np.std(signal[:]))) | (signal >=(threshold*np.std(signal[:])))).astype('int64')\n",
    "\n",
    "\n",
    "    dead_time=(fs/1000)*dead_time\n",
    "\n",
    "    duration=(fs/1000)*duration\n",
    "\n",
    "    pretrig=(fs/1000)*1\n",
    "    postrig=(fs/1000)*2\n",
    "\n",
    "    buffer=20 #1ms to detect aps with positive threshold crossings but not negative\n",
    "\n",
    "    r1=r1[np.where(r1==0)[0][0]:]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    diffr1=np.diff(r1)\n",
    "\n",
    "    ###pretrig is 2 data points, so there could be -1 or -2 at the the start of the trace if so replace with 0\n",
    "\n",
    "\n",
    "    rights=(np.where(diffr1==(1))[0]+postrig).astype('int')\n",
    "    lefts=(np.where(diffr1==1)[0]-pretrig).astype('int')\n",
    "\n",
    "    lefts[(np.where(lefts)<0)[0]]=0\n",
    "\n",
    "\n",
    "\n",
    "    ###\n",
    "\n",
    "    true_rights=np.where(diffr1==(-1))[0]+1\n",
    "    true_lefts=np.where(diffr1==1)[0]\n",
    "\n",
    "\n",
    "    #ranges=np.where(diffr1==(-1))[0]-np.where(diffr1==1)[0]\n",
    "    lefts=lefts[:len(rights)]\n",
    "    rights=rights[:len(lefts)] #\n",
    "    #print(lefts, rights, 'before')\n",
    "\n",
    "    true_lefts=true_lefts[:len(true_rights)]\n",
    "    true_rights=true_rights[:len(true_lefts)]\n",
    "\n",
    "    #in range min index + left original index\n",
    "\n",
    "    #dead time_sufficient\n",
    "    dead_idx=(np.where(np.diff(lefts)<dead_time)[0]+1).tolist()\n",
    "\n",
    "    dead_idx=(np.where(np.diff(true_lefts)<dead_time)[0]+1).tolist()\n",
    "\n",
    "    #print(dead_idx)\n",
    "    lefts=np.delete(lefts, dead_idx)\n",
    "    rights=np.delete(rights, dead_idx)\n",
    "\n",
    "    true_lefts=np.delete(true_lefts, dead_idx)\n",
    "    true_rights=np.delete(true_rights, dead_idx)\n",
    "\n",
    "    #print(lefts, rights, dead_time, rights-lefts, 'deadidx')\n",
    "    ##print(len(lefts))\n",
    "\n",
    "    \"\"\"removing spikes that violate dead time criteria also 50us peaks\"\"\"\n",
    "\n",
    "    while np.any(np.diff(lefts)<dead_time) or np.any((rights-lefts)<1) or np.any((rights-lefts)>maxdur):\n",
    "\n",
    "        dead_idx=(np.where(np.diff(lefts)<dead_time)[0]+1).tolist()\n",
    "        lefts=np.delete(lefts, dead_idx)\n",
    "        rights=np.delete(rights, dead_idx)\n",
    "    #no data\n",
    "        to_delmin=np.where((rights-lefts)<1)[0].tolist()\n",
    "\n",
    "        #print(to_delmin, 'delmin')\n",
    "        lefts=np.delete(lefts, to_delmin)\n",
    "        rights=np.delete(rights, to_delmin)\n",
    "\n",
    "        to_delmax=np.where((rights-lefts)>maxdur)[0].tolist()\n",
    "\n",
    "        #print(to_delmax, 'delmax')\n",
    "        lefts=np.delete(lefts, to_delmax)\n",
    "        rights=np.delete(rights, to_delmax)\n",
    "\n",
    "    #try:\n",
    "\n",
    "    #print(lefts, 'lefts', 'after')\n",
    "    #print(rights, 'rights')\n",
    "\n",
    "    mins=np.array([peak(signal[(lefts[i]):(rights[i])], positive, negative)[0][0]+(lefts[i]) for i in range(len(lefts))\n",
    "                   if len(peak(signal[(lefts[i]):(rights[i])], positive, negative)[0])>0]) #future peak search\n",
    "    ind=[i for i in range(len(lefts)) if len(peak(signal[lefts[i]:(rights[i]+1)], positive, negative)[0])>0]\n",
    "    lefts=lefts[ind]\n",
    "    rights=rights[ind]#future peak search\n",
    "\n",
    "\n",
    "    #print(mins, 'mins')\n",
    "    indexes=[i for i in range(len(mins)) if (((mins[i]-int(duration//2))>0) & ((mins[i]+int(duration//2))<len(signal)))]\n",
    "\n",
    "    #print(mins, 'mins')\n",
    "\n",
    "\n",
    "    mins=mins[indexes]\n",
    "    lefts=lefts[indexes]\n",
    "    rights=rights[indexes]\n",
    "\n",
    "\n",
    "\n",
    "    #except:\n",
    "        #ValueError\n",
    "\n",
    "\n",
    "\n",
    "        #mins=[]\n",
    "\n",
    "\n",
    "    if len(mins)>1:\n",
    "\n",
    "        divide=(mins-true_lefts)\n",
    "        divide[divide==0]=1\n",
    "        rise_vel=(np.abs(signal[mins]-signal[true_lefts]))/divide\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        divide=(true_rights-mins)\n",
    "        divide[divide==0]=1\n",
    "        fall_vel=(np.abs(signal[true_rights]-signal[mins]))/divide\n",
    "\n",
    "        # for testing will be >=1 but for the data >1\n",
    "\n",
    "\n",
    "\n",
    "        spks_wavef=np.array([signal[i-int(duration/2):i+int(duration/2)] for i in mins])\n",
    "\n",
    "\n",
    "        amplitudes=spks_wavef[:, 0]-spks_wavef[:, int(spks_wavef.shape[1]/2)]\n",
    "\n",
    "        features=np.apply_along_axis(wvf_features, 1, spks_wavef)\n",
    "\n",
    "\n",
    "\n",
    "        pca_features=pca(spks_wavef, 4)\n",
    "\n",
    "\n",
    "\n",
    "        X=np.concatenate((rise_vel.reshape(rise_vel.shape[0], 1), fall_vel.reshape(rise_vel.shape[0], 1),\n",
    "                          amplitudes.reshape(amplitudes.shape[0], 1)), axis=1)\n",
    "\n",
    "\n",
    "        lof = load_model('Novelty.pkl')\n",
    "        yhat = lof.predict(X)\n",
    "\n",
    "\n",
    "\n",
    "        ##find novel peaks indexes\n",
    "\n",
    "        #mask = yhat != -1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #X=X[mask, :]\n",
    "        #spks_wavef=spks_wavef[mask, :]\n",
    "        #amplitudes=amplitudes[mask]\n",
    "        ##print(mins, '3rd')\n",
    "\n",
    "\n",
    "\n",
    "        #mins=mins[mask]\n",
    "\n",
    "        #print(mins)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  #u  outliers\n",
    "        ##print(mins)\n",
    "\n",
    "        if len(mins)>0:\n",
    "\n",
    "            plt.figure()\n",
    "            plt.xlabel('rise_vel')\n",
    "            plt.ylabel('Fall_vel')\n",
    "            plt.scatter(rise_vel, fall_vel)\n",
    "            plt.scatter(X[:, 0], X[:, 1], color='yellow', alpha=0.2)\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "    else:\n",
    "        X=np.empty([0, 2])\n",
    "        mins=np.array([])\n",
    "        spks_wavef=np.empty([0, 60])\n",
    "        amplitudes=np.array([])\n",
    "        features=np.empty([0, 24])\n",
    "        pca_features=np.empty([0, 4])\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(signal)\n",
    "\n",
    "\n",
    "    y = np.interp(mins, np.arange(0, len(signal), 1), signal)\n",
    "    ax.plot(mins, y, ls=\"\", marker=\"*\", ms=15,  color=\"crimson\")\n",
    "    plt.axhline(y=(-threshold*np.std(signal[:])))\n",
    "    #plt.savefig(to+'/'+str(len(mins))+'Spikes'+'.tif', format='tif')\n",
    "\n",
    "    #plt.xlim([19950, 20050])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PipNoise_detection(signal, dead_time, threshold, negative, positive, fs, duration):\n",
    "\n",
    "    if positive==False:\n",
    "        r1=(signal <=(-threshold*np.std(signal[:]))).astype('int64')\n",
    "\n",
    "    else:\n",
    "         r1=((signal <=(-threshold*np.std(signal[:]))) | (signal >=(threshold*np.std(signal[:])))).astype('int64')\n",
    "\n",
    "\n",
    "    dead_time=(fs/1000)*dead_time\n",
    "\n",
    "    duration=(fs/1000)*duration\n",
    "\n",
    "    buffer=20 #1ms to detect aps with positive threshold crossings but not negative\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    diffr1=np.diff(r1)\n",
    "    ##print(r1[:5000])\n",
    "    rights=np.where(diffr1==(-1))[0]+1\n",
    "    lefts=np.where(diffr1==1)[0]\n",
    "\n",
    "    #ranges=np.where(diffr1==(-1))[0]-np.where(diffr1==1)[0]\n",
    "    lefts=lefts[:len(rights)]\n",
    "    rights=rights[:len(lefts)] #\n",
    "\n",
    "\n",
    "    #in range min index + left original index\n",
    "\n",
    "    #dead time_sufficient\n",
    "    dead_idx=(np.where(np.diff(lefts)<dead_time)[0]+1).tolist()\n",
    "    lefts=np.delete(lefts, dead_idx)\n",
    "    rights=np.delete(rights, dead_idx)\n",
    "    ##print(len(lefts))\n",
    "\n",
    "    \"\"\"removing spikes that violate dead time criteria also 50us peaks\"\"\"\n",
    "\n",
    "    #try:\n",
    "\n",
    "    #print(lefts, 'lefts')\n",
    "    #print(rights, 'rights')\n",
    "\n",
    "    mins=np.array([peak(signal[(lefts[i]):(rights[i]+1)], positive, negative)[0][0]+(lefts[i]) for i in range(len(lefts))\n",
    "                   if len(peak(signal[(lefts[i]):(rights[i]+1)], positive, negative)[0])>0]) #future peak search\n",
    "    ind=[i for i in range(len(lefts)) if len(peak(signal[lefts[i]:(rights[i]+1)], positive, negative)[0])>0]\n",
    "    lefts=lefts[ind]\n",
    "    rights=rights[ind]#future peak search\n",
    "\n",
    "    indexes=[i for i in range(len(mins)) if (((mins[i]-int(duration/2))>0) & ((mins[i]+int(duration/2))<len(signal)))]\n",
    "    mins=mins[indexes]\n",
    "    lefts=lefts[indexes]\n",
    "    rights=rights[indexes]\n",
    "\n",
    "\n",
    "\n",
    "    if len(mins)>1:\n",
    "\n",
    "        divide=(mins-lefts)\n",
    "        divide[divide==0]=1\n",
    "        rise_vel=(np.abs(signal[mins]-signal[lefts]))/divide\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        divide=(rights-mins)\n",
    "        divide[divide==0]=1\n",
    "        fall_vel=(np.abs(signal[rights]-signal[mins]))/divide\n",
    "\n",
    "        # for testing will be >=1 but for the data >1\n",
    "        \n",
    "        spks_wavef=np.array([signal[i-int(duration/2):i+int(duration/2)] for i in mins])\n",
    "        amplitudes=spks_wavef[:, 0]-spks_wavef[:, int(spks_wavef.shape[1]/2)]\n",
    "\n",
    "\n",
    "\n",
    "        X=np.concatenate((rise_vel.reshape(rise_vel.shape[0], 1), fall_vel.reshape(rise_vel.shape[0], 1),\n",
    "                              amplitudes.reshape(amplitudes.shape[0], 1)), axis=1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        lof = load_model('Novelty.pkl')\n",
    "        yhat = lof.predict(X)\n",
    "\n",
    "\n",
    "\n",
    "        ##find novel peaks indexes\n",
    "        masknoise = yhat == -1\n",
    "\n",
    "        minsnoise=mins[masknoise]\n",
    "        \n",
    "        \n",
    "        if len(minsnoise)>0:\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            spks_wav=np.array([signal[i-int(duration/2):i+int(duration/2)] for i in minsnoise])\n",
    "            amplitudes=spks_wav[:, 0]-spks_wav[:, int(spks_wav.shape[1]/2)]\n",
    "            \n",
    "        else:\n",
    "            X=np.empty([0, 2])\n",
    "            mins=np.array([])\n",
    "            spks_wav=np.empty([0, 60])\n",
    "            amplitudes=np.array([])\n",
    "            features=np.empty([0, 24])\n",
    "            pca_features=np.empty([0, 4])\n",
    "\n",
    "            minsnoise=np.array([])\n",
    "\n",
    "\n",
    "\n",
    "       \n",
    "\n",
    "    else:\n",
    "        X=np.empty([0, 2])\n",
    "        mins=np.array([])\n",
    "        spks_wav=np.empty([0, 60])\n",
    "        amplitudes=np.array([])\n",
    "        features=np.empty([0, 24])\n",
    "        pca_features=np.empty([0, 4])\n",
    "\n",
    "        minsnoise=np.array([])\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(signal, color='blue')\n",
    "\n",
    "    if len(mins)>0:\n",
    " \n",
    "        if len(minsnoise)>0:\n",
    "\n",
    "\n",
    "            y = np.interp(minsnoise, np.arange(0, len(signal), 1), signal)\n",
    "            ax.plot(minsnoise, y, ls=\"\", marker=\"*\", ms=15,  color=\"crimson\")\n",
    "            plt.axhline(y=(-threshold*np.std(signal[:])))\n",
    "            plt.title('PIpnoiseplot')\n",
    "            #plt.savefig(to+'/'+str(len(mins))+'Spikes'+'.tif', format='tif')\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return (spks_wav, minsnoise, amplitudes, X, threshold*np.std(signal[:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "LhTYJFLrZHtp",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return b, a\n",
    "\n",
    "\n",
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    y = lfilter(b, a, data)\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "vuPWEJDwZHtp",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def highpass(v, cutoff, order, fs):\n",
    "\n",
    "    nyq = 0.5 * 20000\n",
    "\n",
    "    normal_cutoff = cutoff/nyq\n",
    "    b, a = signal.butter(order, normal_cutoff, btype='high', analog=False)\n",
    "\n",
    "    filtered = signal.filtfilt(b, a, v)\n",
    "\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "IQrNUFhaZHtq",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def step_fit(t, response):\n",
    "    step_mod=StepModel(form='linear', prefix='step_', nan_policy='omit') #step function\n",
    "    #index=np.where(np.array(response)==np.array(response.min()))[0][0] #we dont need this\n",
    "    ##print(index)\n",
    "    scale=response.std()/response.mean()\n",
    "    pars = step_mod.make_params(step_center=t.mean(),\n",
    "                         step_amplitude=response.std(),\n",
    "                         step_sigma=np.abs(scale)*1.5)  # parameters for the step function sigma=1.4 best parameter\n",
    "    out = step_mod.fit(response, pars, x=t)\n",
    "\n",
    "    return out.best_fit\n",
    "\n",
    "\n",
    "def salpa(t, *p):\n",
    "    return p[0]+(p[1])*(t)+(p[2])*((t)**2)+(p[3])*((t)**3)  #curve function\n",
    "\n",
    "\n",
    "def salpa_fit(v, centroid):\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(v)\n",
    "    plt.title('Raw')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    d=len(v)-(centroid)  #get upper range of\n",
    "\n",
    "    t=np.arange(-int(centroid/2), int(centroid/2)).astype('int64')\n",
    "    #print(t.shape)#centered on zero\n",
    "\n",
    "    response=v[:len(t)]\n",
    "    #print(response.shape)#\n",
    "\n",
    "    y=step_fit(t, response).tolist()\n",
    "\n",
    "    nc=np.arange(int(centroid), d).astype('int64') #get centroids\n",
    "\n",
    "\n",
    "    for n in nc:\n",
    "\n",
    "        t=np.arange(-int(centroid), int(centroid)).astype('int64') #arange input from -centroid to centroid, around zero\n",
    "\n",
    "        response=v[int(n-centroid):int(n+centroid)]  #get signal window\n",
    "\n",
    "\n",
    "\n",
    "        params, params_covariance = optimize.curve_fit(salpa, t, response, p0=np.ones(4))\n",
    "        #optimize the function\n",
    "\n",
    "        y_nc=salpa(t, *params)  #predict the central point value\n",
    "\n",
    "        y.append(y_nc[int(centroid)]) #append only central point to list\n",
    "\n",
    "\n",
    "    dv=np.array(v[: len(y)])-np.array(y)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(dv)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    return dv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "V2p-cDGaZHtq",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Dose(event_entities):\n",
    "\n",
    "    starts=[]\n",
    "    stops=[]\n",
    "\n",
    "    #print(event_entities.keys())\n",
    "\n",
    "    for e in list(event_entities.keys()):\n",
    "        ##print('key', e)\n",
    "\n",
    "\n",
    "        label_info=event_entities[e].info.__dict__['info']['Label']\n",
    "        ##print('label', label_info)\n",
    "\n",
    "        time=event_entities[e].data[0]\n",
    "\n",
    "        ##print('time', time)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if re.search('ControlStart', label_info) or re.search('TestStart', label_info):\n",
    "            starts.extend(time)\n",
    "\n",
    "        if re.search('ControlStop', label_info) or re.search('TestStop', label_info):\n",
    "            stops.extend(time)\n",
    "\n",
    "    return starts, stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "YjnLy0I3ZHtr",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def RArtifact(v, centroid, cutoff, order, fs, alg='StepSalpa'):\n",
    "\n",
    "    #p = figure(title=\"Electrical Noise\", x_axis_label='x', y_axis_label='y')\n",
    "\n",
    "\n",
    "\n",
    "    if alg=='StepSalpa':\n",
    "\n",
    "\n",
    "        dv=salpa_fit(v, centroid)\n",
    "\n",
    "    else:\n",
    "\n",
    "\n",
    "        dv=highpass(v, cutoff, order, fs)\n",
    "\n",
    "    #p.line(np.arange(0, len(v), 1), v, legend_label=\"noise\", line_width=2, color='red')\n",
    "    #p.line(np.arange(0, len(dv), 1), dv, legend_label=\"Salpa\", line_width=2, color='blue')\n",
    "    #show(p)\n",
    "    return dv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4N3K5ip-ZHts"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "LNqeO8x8ZHts",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def SpikeDetection_EStim(to, lof, Recordingfilename, MetadataFilename, MetadataMaskPath, experiments, Analyze,\n",
    "                          \n",
    "                    detmethod, E_Stim,\n",
    "                         fs, cutoff, highpassth, order, dead_time, threshold,\n",
    "                          usewelllist, negative, positive,\n",
    "                          PipNoise, noallwells, mindur, maxdur, onlyLed):\n",
    "\n",
    "\n",
    "    \"\"\"Function to detect spike and write spike detections in a table for a single recording file\"\"\"\n",
    "\n",
    "    D={'Timestamp [µs]':[], 'Peak Amplitudes':[], 'Duration':[], 'Channel ID':[], 'Well ID':[], 'Well Label':[],\n",
    "       'Channel Label':[], 'Experiment':[], 'Dose Label':[], 'Compound ID':[], 'ES_condition':[], \n",
    "       'Threshold':[], 'BaselineM':[]}\n",
    "    \n",
    "    \n",
    "    F=np.empty([0, 28])\n",
    "    W=np.empty([1, 1])\n",
    "\n",
    "    recording=McsPy.McsData.RawData(Recordingfilename)\n",
    "\n",
    "    tree = etree.parse(MetadataFilename)\n",
    "    WellID=np.load(MetadataMaskPath+'/'+'WellID.csv.npy')\n",
    "    Welllabel=np.load(MetadataMaskPath+'/'+'WellLabel.csv.npy')\n",
    "    Channellabel=np.load(MetadataMaskPath+'/'+'ChannelLabel.csv.npy')\n",
    "\n",
    "    ChannelID=list(recording.recordings[0].analog_streams[0].channel_infos.keys())\n",
    "\n",
    "    ChannelID=[int(i) for i in ChannelID]\n",
    "    Compounds=tree.xpath('//CompoundID/text()')\n",
    "    #print(Compounds)\n",
    "    Compound=recording.comment\n",
    "\n",
    "    Labels=Dilutions(tree, Compound)\n",
    "    lab=lab_book(tree)\n",
    "\n",
    "    lof_trained=0\n",
    "\n",
    "\n",
    "\n",
    "    ES_Times=[]\n",
    "    ES_Amplitudes=[]\n",
    "    ES_Durations=[]\n",
    "\n",
    "    ExperimentID=tree.xpath('//ExperimentID/text()')[0]\n",
    "    ES_Electrodes=tree.xpath('//ChannelID/text()')\n",
    "    ES_Electrodes=[int(i) for i in ES_Electrodes]\n",
    "    ES_Wells=np.unique(np.array([WellID[int(i)] for i in ES_Electrodes]))\n",
    "    ES_all=[np.where(WellID==i)[0].tolist()[:] for i in ES_Wells]\n",
    "    ES_all=[item for sublist in ES_all for item in sublist]\n",
    "\n",
    "    exp_times=sorted(experiments[ExperimentID][Compound])\n",
    "    if onlyLed==True:\n",
    "        exp_times=np.unique(np.array(exp_times)).tolist()\n",
    "\n",
    "\n",
    "\n",
    "    starts=recording.recordings[0].analog_streams[1].timestamp_index[:, 1]\n",
    "    stops=recording.recordings[0].analog_streams[1].timestamp_index[:, 2]\n",
    "    esim=recording.recordings[0].analog_streams[1].timestamp_index[:, 0]\n",
    "    #print(esim, exp_times)\n",
    "    #dose start in us\n",
    "\n",
    "    if len(ES_Electrodes)>0:\n",
    "\n",
    "        if len(list(recording.recordings[0].event_streams[0].event_entity.keys()))>0:\n",
    "\n",
    "            ES=recording.recordings[0].event_streams[0].event_entity\n",
    "\n",
    "            key=list(ES.keys())[0]\n",
    "            ES_Times=((ES[key].__dict__['data'][0])).astype('int64')\n",
    "\n",
    "            ES_Amplitudes=tree.xpath('//Phase2Amplitude/text()')[0]\n",
    "\n",
    "            ES_Amplitudes=np.array([int(i) for i in ES_Amplitudes.split(' ')])\n",
    "\n",
    "\n",
    "            ES_Times=ES_Times[0::1]\n",
    "            ES_imes=[(ES_Times[(ES_Times >=esim[i]) & (ES_Times<esim[i+1])]-esim[i])/50  if i!=len(esim)-1 else (ES_Times[ES_Times>esim[i]]-esim[i])/50  for i in range(len(esim))]\n",
    "            ES_mes=[(ES_Times[(ES_Times >=esim[i]) & (ES_Times<esim[i+1])]) if i!=len(esim)-1 else (ES_Times[ES_Times>esim[i]])  for i in range(len(esim))]\n",
    "            ES_Durations=tree.xpath('//Phase2Duration/text()')[0]\n",
    "            ES_Durations=[int(i) for i in ES_Durations.split(' ')][0]\n",
    "\n",
    "\n",
    "    dict_ES={}\n",
    "\n",
    "\n",
    "\n",
    "    dict_ES['All_Electrodes']=ES_all  #all electrodes in stim wells\n",
    "    dict_ES['Amplitudes']=ES_Amplitudes\n",
    "    dict_ES['Wells']=ES_Wells\n",
    "    dict_ES['Electrodes']=ES_Electrodes #only stim electrodes\n",
    "    dict_ES['Times']=ES_Times\n",
    "\n",
    "\n",
    "    #rint(ES_imes)\n",
    "\n",
    "\n",
    "    #print('starts',starts)\n",
    "    #print('stops', stops)\n",
    "    #print('Labels', Labels)\n",
    "\n",
    "    stamp_dict={}\n",
    "\n",
    "    sample_start=0\n",
    "\n",
    "    for j in range(len(starts)):\n",
    "\n",
    "        #dose_label=Labels[j]\n",
    "        dose_start=int(starts[j])\n",
    "        dose_stop=int(stops[j])\n",
    "\n",
    "        dose_st=esim[j]\n",
    "        #print('exp_times', exp_times)\n",
    "        dose_label=[Labels[i] for i in range(len(exp_times)) if exp_times[i]==dose_st]\n",
    "        dose_et=esim[j]+(50*(dose_stop-dose_start))\n",
    "        stamp_dict[str(dose_label)]={'start':dose_st, 'stop':dose_et}\n",
    "        #print(dose_start, dose_stop, dose_label)\n",
    "\n",
    "        if len(ES_Times)>0:\n",
    "\n",
    "            ES_T=(ES_imes[j]+sample_start).astype('int64')\n",
    "            ES_es=(ES_mes[j]).astype('int64')\n",
    "\n",
    "\n",
    "\n",
    "            sample_start=dose_stop\n",
    "\n",
    "            if len(ES_T)>0:\n",
    "\n",
    "                centroid=int((ES_T[-1]-ES_T[-2])/100)\n",
    "                #print('centroid', centroid)\n",
    "\n",
    "                window=int(ES_T[-1]-ES_T[-2])-centroid\n",
    "                #print('window', window)\n",
    "\n",
    "        artifact={}\n",
    "\n",
    "        if Analyze=='True':\n",
    "\n",
    "            for i in ChannelID:\n",
    "\n",
    "                #print(i, 'channel id')\n",
    "\n",
    "\n",
    "                index=int(recording.recordings[0].analog_streams[0].channel_infos[i].row_index)\n",
    "\n",
    "                #print(Welllabel[i], 'well')\n",
    "\n",
    "                artifact[i]=[]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                if ((i in ES_all) and (len(ES_T)))>0:\n",
    "\n",
    "                    if i in ES_Electrodes: ##future save this info\n",
    "                        print('Stimulation')\n",
    "\n",
    "\n",
    "\n",
    "                    v=recording.recordings[0].analog_streams[0].channel_data[index, dose_start:ES_T[0]]\n",
    "\n",
    "                    fv=highpass(v, highpassth, 3, 20000)\n",
    "\n",
    "\n",
    "                    APs=detmethod(to, lof, fv, dead_time, threshold, negative, positive,\n",
    "                                              fs, mindur, False, start=dose_start)\n",
    "\n",
    "\n",
    "                    D=sd_extension(D, i,  APs, ExperimentID, Welllabel, WellID, Channellabel, Compound, dose_label, 0)\n",
    "\n",
    "\n",
    "                    v=recording.recordings[0].analog_streams[0].channel_data[index, (ES_T[-1]+window):dose_stop]\n",
    "\n",
    "\n",
    "\n",
    "                    fv=highpass(v, highpassth, 3, 20000)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    APs=detmethod(to, lof, fv, dead_time, threshold, negative, positive, fs, mindur, False, start=(ES_T[-1]+window))\n",
    "\n",
    "                    D=sd_extension(D, i,  APs, ExperimentID, Welllabel, WellID, Channellabel, Compound, dose_label, 2)\n",
    "\n",
    "\n",
    "                    for t in range(len(ES_T)):\n",
    "\n",
    "                        if E_Stim==True:\n",
    "\n",
    "\n",
    "                            time=ES_T[t]\n",
    "\n",
    "                            t_l=ES_es[t]\n",
    "\n",
    "                            if t!=len(ES_T)-1:\n",
    "\n",
    "                                centroid=int((ES_T[t+1]-ES_T[t])/100)\n",
    "\n",
    "\n",
    "                                window=int(ES_T[t+1]-ES_T[t])-centroid\n",
    "\n",
    "                            else:\n",
    "                                centroid=int((ES_T[t]-ES_T[t-1])/100)\n",
    "\n",
    "\n",
    "                                window=int(ES_T[t]-ES_T[t-1])-centroid\n",
    "\n",
    "\n",
    "                            centroid=50\n",
    "\n",
    "                            v=recording.recordings[0].analog_streams[0].channel_data[index, (time):(time+window+centroid)]\n",
    "\n",
    "\n",
    "\n",
    "                            dv=RArtifact(v, centroid, cutoff, order, fs, alg='StepSalpa')\n",
    "\n",
    "\n",
    "                            #fv=highpass(dv, 5, 3, 20000)\n",
    "\n",
    "\n",
    "                            maxart=max(abs(v))-min(abs(v))\n",
    "\n",
    "                            artifact[i].append(maxart)\n",
    "\n",
    "\n",
    "\n",
    "                            #APs=AP_detection_lof(to, lof, dv[100:220], dead_time,\n",
    "                                                      #threshold, negative, positive, fs, 2, False, start=(time+100))\n",
    "\n",
    "\n",
    "\n",
    "                            #D=sd_extension(D, i,  APs, ExperimentID, Welllabel, WellID, Channellabel, Compound, dose_label, t_l)\n",
    "\n",
    "                            APs=detmethod(to, lof,  dv[100:], dead_time, threshold, negative,\n",
    "                                                      positive, fs, 2, False, start=(time)+100)\n",
    "\n",
    "                            D=sd_extension(D, i,  APs, ExperimentID, Welllabel, WellID, Channellabel, Compound, dose_label, t_l+11000)\n",
    "\n",
    "\n",
    "\n",
    "                else:\n",
    "\n",
    "                    #print('here')\n",
    "\n",
    "                    #print(dose_label, 'dose')\n",
    "\n",
    "                    #print(ExperimentID, 'experiment')\n",
    "                    \n",
    "\n",
    "                    v=recording.recordings[0].analog_streams[0].channel_data[index, dose_start:dose_stop]\n",
    "                    #fv=butter_bandpass_filter(v, 10, 3500,  20000, order=3)\n",
    "                    fv=highpass(v, highpassth, 3, 20000)\n",
    "                    \n",
    "                    \n",
    "                    APs=detmethod(to, lof, fv, dead_time, threshold,\n",
    "                                         negative, positive, fs,  mindur, True, start=dose_st/50)\n",
    "\n",
    "                    #print(dose_st/50, 'start_input')\n",
    "                    #print(len(APs[1]))\n",
    "                    \n",
    "                    D=sd_extension(D, i,  APs, ExperimentID, Welllabel, WellID, Channellabel, Compound, dose_label, 0)\n",
    "                    lof=APs[4]\n",
    "                    features=APs[5]\n",
    "                    #print(W.shape, 'W')\n",
    "                    #print(APs[0].shape, 'New w shape')\n",
    "                    if W.shape[1]>2:\n",
    "                        W=np.concatenate((W, APs[0]), axis=0)\n",
    "                    else:\n",
    "                        W=APs[0]\n",
    "\n",
    "                    F=np.concatenate((F, features), axis=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        lof_trained=lof\n",
    "\n",
    "        dict_ES['Artifact Values']=artifact\n",
    "\n",
    "        for key in list(D.keys()):\n",
    "            \n",
    "            print(len(D[key]), key)\n",
    "\n",
    "\n",
    "\n",
    "    return (pd.DataFrame(D), dict_ES, lof_trained, lab, stamp_dict, F, W, {})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ES_info(recording, tree, experiments):\n",
    "    \n",
    "    ES_Times=[]\n",
    "    ES_Amplitudes=[]\n",
    "    ES_Durations=[]\n",
    "    \n",
    "    dict_ES={}\n",
    "    \n",
    "    ExperimentID=tree.xpath('//ExperimentID/text()')[0]\n",
    "    \n",
    "    ES_Electrodes=tree.xpath('//ChannelID/text()')\n",
    "    ES_Electrodes=[int(i) for i in ES_Electrodes]\n",
    "    ES_Wells=np.unique(np.array([WellID[int(i)] for i in ES_Electrodes]))\n",
    "    ES_all=[np.where(WellID==i)[0].tolist()[:] for i in ES_Wells]\n",
    "    ES_all=[item for sublist in ES_all for item in sublist]\n",
    "\n",
    "    exp_times=sorted(experiments[ExperimentID][Compound])\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    starts=recording.recordings[0].analog_streams[1].timestamp_index[:, 1]\n",
    "    stops=recording.recordings[0].analog_streams[1].timestamp_index[:, 2]\n",
    "    esim=recording.recordings[0].analog_streams[1].timestamp_index[:, 0]\n",
    "    #print(esim, exp_times)\n",
    "    #dose start in us\n",
    "\n",
    "    if len(ES_Electrodes)>0:\n",
    "\n",
    "        if len(list(recording.recordings[0].event_streams[0].event_entity.keys()))>0:\n",
    "\n",
    "            ES=recording.recordings[0].event_streams[0].event_entity\n",
    "\n",
    "            key=list(ES.keys())[0]\n",
    "            ES_Times=((ES[key].__dict__['data'][0])).astype('int64')\n",
    "\n",
    "            ES_Amplitudes=tree.xpath('//Phase2Amplitude/text()')[0]\n",
    "\n",
    "            ES_Amplitudes=np.array([int(i) for i in ES_Amplitudes.split(' ')])\n",
    "\n",
    "\n",
    "            ES_Times=ES_Times[0::1]\n",
    "            ES_imes=[(ES_Times[(ES_Times >=esim[i]) & (ES_Times<esim[i+1])]-esim[i])/50  if i!=len(esim)-1 else (ES_Times[ES_Times>esim[i]]-esim[i])/50  for i in range(len(esim))]\n",
    "            ES_mes=[(ES_Times[(ES_Times >=esim[i]) & (ES_Times<esim[i+1])]) if i!=len(esim)-1 else (ES_Times[ES_Times>esim[i]])  for i in range(len(esim))]\n",
    "            ES_Durations=tree.xpath('//Phase2Duration/text()')[0]\n",
    "            ES_Durations=[int(i) for i in ES_Durations.split(' ')][0]\n",
    "\n",
    "\n",
    "    dict_ES['All_Electrodes']=ES_all  #all electrodes in stim wells\n",
    "    dict_ES['Amplitudes']=ES_Amplitudes\n",
    "    dict_ES['Wells']=ES_Wells\n",
    "    dict_ES['Electrodes']=ES_Electrodes #only stim electrodes\n",
    "    dict_ES['Times']=ES_Times\n",
    "    \n",
    "    \n",
    "    return dict_ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "LNqeO8x8ZHts",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Raw(to, Recordingfilename, MetadataFilename, MetadataMaskPath, experiments, E_Stim, onlyLed):\n",
    "\n",
    "\n",
    "    \"\"\"Function to detect spike and write spike detections in a table for a single recording file\"\"\"\n",
    "\n",
    "    \n",
    "    \n",
    "   \n",
    "\n",
    "    recording=McsPy.McsData.RawData(Recordingfilename)\n",
    "\n",
    "    tree = etree.parse(MetadataFilename)\n",
    "    WellID=np.load(MetadataMaskPath+'/'+'WellID.csv.npy')\n",
    "    Welllabel=np.load(MetadataMaskPath+'/'+'WellLabel.csv.npy')\n",
    "    Channellabel=np.load(MetadataMaskPath+'/'+'ChannelLabel.csv.npy')\n",
    "\n",
    "    ChannelID=list(recording.recordings[0].analog_streams[0].channel_infos.keys())\n",
    "\n",
    "    ChannelID=[int(i) for i in ChannelID]\n",
    "    Compounds=tree.xpath('//CompoundID/text()')\n",
    "    #print(Compounds)\n",
    "    Compound=recording.comment\n",
    "\n",
    "    Labels=Dilutions(tree, Compound)\n",
    "    lab=lab_book(tree)\n",
    "    \n",
    "    ExperimentID=tree.xpath('//ExperimentID/text()')[0]\n",
    "    \n",
    "    exp_times=sorted(experiments[ExperimentID][Compound])\n",
    "    \n",
    "    \n",
    "    if onlyLed==True:\n",
    "        exp_times=np.unique(np.array(exp_times)).tolist()\n",
    "\n",
    "\n",
    "\n",
    "    starts=recording.recordings[0].analog_streams[1].timestamp_index[:, 1]\n",
    "    stops=recording.recordings[0].analog_streams[1].timestamp_index[:, 2]\n",
    "    esim=recording.recordings[0].analog_streams[1].timestamp_index[:, 0]\n",
    "    #print(esim, exp_times)\n",
    "    #dose start in us\n",
    "    \n",
    "    dict_ES=ES_info(recording, tree, experiments)\n",
    "    \n",
    "    stamp_dict={}\n",
    "\n",
    "    sample_start=0\n",
    "    \n",
    "    \n",
    "    ##if drug experiment, list of dose starts and stops, so N(doses) arrays.\n",
    "\n",
    "    for j in range(len(starts)):\n",
    "\n",
    "        #dose_label=Labels[j]\n",
    "        dose_start=int(starts[j])\n",
    "        dose_stop=int(stops[j])\n",
    "\n",
    "        dose_st=esim[j]\n",
    "        #print('exp_times', exp_times)\n",
    "        dose_label=[Labels[i] for i in range(len(exp_times)) if exp_times[i]==dose_st]\n",
    "        dose_et=esim[j]+(50*(dose_stop-dose_start))\n",
    "        stamp_dict[str(dose_label)]={'start':dose_st, 'stop':dose_et}\n",
    "        #print(dose_start, dose_stop, dose_label)\n",
    "       \n",
    "\n",
    "        \n",
    "        for i in ChannelID:\n",
    "            \n",
    "            index=int(recording.recordings[0].analog_streams[0].channel_infos[i].row_index)\n",
    "\n",
    "              \n",
    "            v=recording.recordings[0].analog_streams[0].channel_data[index, dose_start:dose_stop]\n",
    "            \n",
    "            ##indtead of yield could be spike detection function \n",
    "            \n",
    "            \n",
    "            yield  v \n",
    "                    \n",
    "\n",
    "       \n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "LNqeO8x8ZHts",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def NSpikeDetection_EStim(to, lof, Recordingfilename, MetadataFilename, MetadataMaskPath, experiments, Analyze,\n",
    "                          \n",
    "                    detmethod, E_Stim,\n",
    "                         fs, cutoff, highpassth, order, dead_time, threshold,\n",
    "                          usewelllist, negative, positive,\n",
    "                          PipNoise, noallwells, mindur, maxdur, onlyLed):\n",
    "\n",
    "\n",
    "    \"\"\"Function to detect spike and write spike detections in a table for a single recording file\"\"\"\n",
    "\n",
    "    D={'Timestamp [µs]':[], 'Peak Amplitudes':[], 'Duration':[], 'Channel ID':[], 'Well ID':[], 'Well Label':[],\n",
    "       'Channel Label':[], 'Experiment':[], 'Dose Label':[], 'Compound ID':[], 'ES_condition':[], \n",
    "       'Threshold':[], 'BaselineM':[]}\n",
    "    \n",
    "    \n",
    "    F=np.empty([0, 28])\n",
    "    W=np.empty([1, 1])\n",
    "\n",
    "    recording=McsPy.McsData.RawData(Recordingfilename)\n",
    "\n",
    "    tree = etree.parse(MetadataFilename)\n",
    "    WellID=np.load(MetadataMaskPath+'/'+'WellID.csv.npy')\n",
    "    Welllabel=np.load(MetadataMaskPath+'/'+'WellLabel.csv.npy')\n",
    "    Channellabel=np.load(MetadataMaskPath+'/'+'ChannelLabel.csv.npy')\n",
    "\n",
    "    ChannelID=list(recording.recordings[0].analog_streams[0].channel_infos.keys())\n",
    "\n",
    "    ChannelID=[int(i) for i in ChannelID]\n",
    "    \n",
    "    if noallwells==True:\n",
    "        \n",
    "        print('is it true?')\n",
    "        \n",
    "        analysis_chlist=np.where(np.isin(WellID.astype('int'), usewelllist)==True)[0]\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        analysis_chlist=ChannelID\n",
    "        \n",
    "        \n",
    "    Compounds=tree.xpath('//CompoundID/text()')\n",
    "    #print(Compounds)\n",
    "    Compound=recording.comment\n",
    "\n",
    "    Labels=Dilutions(tree, Compound)\n",
    "    lab=lab_book(tree)\n",
    "\n",
    "    lof_trained=0\n",
    "\n",
    "\n",
    "\n",
    "    ES_Times=[]\n",
    "    ES_Amplitudes=[]\n",
    "    ES_Durations=[]\n",
    "\n",
    "    ExperimentID=tree.xpath('//ExperimentID/text()')[0]\n",
    "    ES_Electrodes=tree.xpath('//ChannelID/text()')\n",
    "    ES_Electrodes=[int(i) for i in ES_Electrodes]\n",
    "    ES_Wells=np.unique(np.array([WellID[int(i)] for i in ES_Electrodes]))\n",
    "    ES_all=[np.where(WellID==i)[0].tolist()[:] for i in ES_Wells]\n",
    "    ES_all=[item for sublist in ES_all for item in sublist]\n",
    "\n",
    "    exp_times=sorted(experiments[ExperimentID][Compound])\n",
    "    if onlyLed==True:\n",
    "        exp_times=np.unique(np.array(exp_times)).tolist()\n",
    "\n",
    "\n",
    "\n",
    "    starts=recording.recordings[0].analog_streams[1].timestamp_index[:, 1]\n",
    "    stops=recording.recordings[0].analog_streams[1].timestamp_index[:, 2]\n",
    "    esim=recording.recordings[0].analog_streams[1].timestamp_index[:, 0]\n",
    "    #print(esim, exp_times)\n",
    "    #dose start in us\n",
    "\n",
    "    if len(ES_Electrodes)>0:\n",
    "\n",
    "        if len(list(recording.recordings[0].event_streams[0].event_entity.keys()))>0:\n",
    "\n",
    "            ES=recording.recordings[0].event_streams[0].event_entity\n",
    "\n",
    "            key=list(ES.keys())[0]\n",
    "            ES_Times=((ES[key].__dict__['data'][0])).astype('int64')\n",
    "\n",
    "            ES_Amplitudes=tree.xpath('//Phase2Amplitude/text()')[0]\n",
    "\n",
    "            ES_Amplitudes=np.array([int(i) for i in ES_Amplitudes.split(' ')])\n",
    "\n",
    "\n",
    "            ES_Times=ES_Times[0::1]\n",
    "            ES_imes=[(ES_Times[(ES_Times >=esim[i]) & (ES_Times<esim[i+1])]-esim[i])/50  if i!=len(esim)-1 else (ES_Times[ES_Times>esim[i]]-esim[i])/50  for i in range(len(esim))]\n",
    "            ES_mes=[(ES_Times[(ES_Times >=esim[i]) & (ES_Times<esim[i+1])]) if i!=len(esim)-1 else (ES_Times[ES_Times>esim[i]])  for i in range(len(esim))]\n",
    "            ES_Durations=tree.xpath('//Phase2Duration/text()')[0]\n",
    "            ES_Durations=[int(i) for i in ES_Durations.split(' ')][0]\n",
    "\n",
    "\n",
    "    dict_ES={}\n",
    "\n",
    "\n",
    "\n",
    "    dict_ES['All_Electrodes']=ES_all  #all electrodes in stim wells\n",
    "    dict_ES['Amplitudes']=ES_Amplitudes\n",
    "    dict_ES['Wells']=ES_Wells\n",
    "    dict_ES['Electrodes']=ES_Electrodes #only stim electrodes\n",
    "    dict_ES['Times']=ES_Times\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "    stamp_dict={}\n",
    "\n",
    "    sample_start=0\n",
    "    \n",
    "    artifacts={}\n",
    "\n",
    "    for j in range(len(starts)):\n",
    "\n",
    "        #dose_label=Labels[j]\n",
    "        dose_start=int(starts[j])\n",
    "        dose_stop=int(stops[j])\n",
    "\n",
    "        dose_st=esim[j]\n",
    "        #print('exp_times', exp_times)\n",
    "        dose_label=[Labels[i] for i in range(len(exp_times)) if exp_times[i]==dose_st]\n",
    "        dose_et=esim[j]+(50*(dose_stop-dose_start))\n",
    "        stamp_dict[str(dose_label)]={'start':dose_st, 'stop':dose_et}\n",
    "        #print(dose_start, dose_stop, dose_label)\n",
    "\n",
    "        if len(ES_Times)>0:\n",
    "\n",
    "            ES_T=(ES_imes[j]+sample_start).astype('int64')\n",
    "            ES_es=(ES_mes[j]).astype('int64')\n",
    "\n",
    "\n",
    "\n",
    "            sample_start=dose_stop\n",
    "\n",
    "            if len(ES_T)>0:\n",
    "\n",
    "                centroid=int((ES_T[-1]-ES_T[-2])/100)\n",
    "                #print('centroid', centroid)\n",
    "\n",
    "                window=int(ES_T[-1]-ES_T[-2])-centroid\n",
    "                #print('window', window)\n",
    "\n",
    "        artifact={}\n",
    "\n",
    "        if Analyze=='True':\n",
    "            \n",
    "            \n",
    "            \n",
    "            #print(analysis_chlist, 'analysislist')\n",
    "\n",
    "            for i in ChannelID:\n",
    "                \n",
    "\n",
    "                \n",
    "                \n",
    "                if i in analysis_chlist:\n",
    "\n",
    "\n",
    "                    index=int(recording.recordings[0].analog_streams[0].channel_infos[i].row_index)\n",
    "\n",
    "                    print(Welllabel[i], 'well')\n",
    "\n",
    "                    artifact[i]=[]\n",
    "                    if PipNoise==True:\n",
    "                        \n",
    "                        print('here', index)\n",
    "\n",
    "                        \n",
    "                        v=recording.recordings[0].analog_streams[0].channel_data[index, dose_start:dose_stop]\n",
    "                        \n",
    "                        plt.figure()\n",
    "                        plt.plot(v)\n",
    "                        plt.title('raw')\n",
    "                        plt.show()\n",
    "                        #fv=butter_bandpass_filter(v, 10, 3500,  20000, order=3)\n",
    "                        fv=highpass(v, highpassth, 3, 20000)\n",
    "\n",
    "                        output=PipNoise_detection(v, 0.5, 5, True, True, 20000, 3)\n",
    "                        noiseindexes=sorted(output[1])\n",
    "                        noiseindexes=np.array(noiseindexes)\n",
    "                        \n",
    "                        \n",
    "                       \n",
    "                        \n",
    "                        if len(noiseindexes)>0:\n",
    "                            \n",
    "                            wind=30000\n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            noiseindexesu=[noiseindexes[0]]+noiseindexes[np.where(np.diff(noiseindexes)>wind)[0]+1].tolist()\n",
    "                            \n",
    "                            artifact[i]=noiseindexesu\n",
    "                            \n",
    "                            print(noiseindexesu, 'noiseindexesu')\n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            for ni, n in enumerate(noiseindexesu):\n",
    "                                \n",
    "                                \n",
    "                                \n",
    "                                    \n",
    "                                    \n",
    "                                if ni!=len(noiseindexesu)-1:\n",
    "                                    \n",
    "                                    endn=noiseindexesu[ni+1]-wind\n",
    "                                    \n",
    "                                else:\n",
    "                                    endn=len(v)\n",
    "                                \n",
    "                                if ni!=0:\n",
    "                                    \n",
    "                                    startn=noiseindexesu[ni-1]+wind\n",
    "                                    \n",
    "                                else:\n",
    "                                    startn=0\n",
    "                                    \n",
    "                                print('heretoo')\n",
    "\n",
    "                                \n",
    "\n",
    "                                noise=v[n-wind:n+wind]\n",
    "                                \n",
    "                                signal1=v[startn:n-wind]\n",
    "                                signal2=v[n+wind:endn]\n",
    "                                \n",
    "                                \n",
    "                                plt.figure()\n",
    "                                \n",
    "                                plt.plot(noise)\n",
    "                                \n",
    "                                plt.title('noise')\n",
    "                                plt.show()\n",
    "                                \n",
    "                                \n",
    "                                startsig1=startn\n",
    "                                startnoise=n-wind\n",
    "                                startsig2=n+wind\n",
    "                                \n",
    "                                if len(signal1)<500:\n",
    "                                    noise=np.concatenate([signal1, noise])\n",
    "                                    signal1=[]\n",
    "\n",
    "                                    startnoise=startsig1\n",
    "                                    \n",
    "                                if len(signal2)<500:\n",
    "                                    \n",
    "\n",
    "                                    noise=np.concatenate([noise, signal2])\n",
    "\n",
    "                                    signal2=[]\n",
    "\n",
    "                                    #startnoise=startsig2\n",
    "                                    \n",
    "                                    \n",
    "                                \n",
    "                                for sig, startnf  in zip([signal1, noise, signal2], [startsig1, startnoise, startsig2]):\n",
    "                                    \n",
    "                                    \n",
    "                                    if len(sig)>0:\n",
    "                                        \n",
    "                                        sigclean=RArtifact(sig, 5, 100, 9, 20000, alg='esim')\n",
    "                                        \n",
    "                                        APs=AP_detection_lofnewest(to, lof, sigclean, 1000,  0.5, 4.8, True,True, 20000, \n",
    "                                                            3, False, True, start=dose_st/50+startnf)\n",
    "                                        \n",
    "                                        D=sd_extension(D, i,  APs, ExperimentID,\n",
    "                                                   Welllabel, WellID, Channellabel, Compound, dose_label, 0)\n",
    "                                        lof=APs[4]\n",
    "                                        features=APs[5]\n",
    "\n",
    "                                        if W.shape[1]>2:\n",
    "                                            W=np.concatenate((W, APs[0]), axis=0)\n",
    "                                        else:\n",
    "                                            W=APs[0]\n",
    "\n",
    "                                        F=np.concatenate((F, features), axis=0)\n",
    "                                        \n",
    "                                        \n",
    "\n",
    "                   \n",
    "                        \n",
    "                        \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    artifacts[str(dose_label)]=artifact\n",
    "    \n",
    "    \n",
    "    lof_trained=lof\n",
    "\n",
    "    dict_ES['Artifact Values']=artifact\n",
    "\n",
    "    for key in list(D.keys()):\n",
    "\n",
    "        print(len(D[key]), key)\n",
    "\n",
    "\n",
    "\n",
    "    return (pd.DataFrame(D), dict_ES, lof_trained, lab, stamp_dict, F, W, artifacts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "TS-xBvAsZHtu",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def save_model(clf, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(clf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "AsPl4JZMZHtu",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_model(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        clf = pickle.load(f)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "HoEePw3KZHtu",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Dose_Label(experiments, folder_path, filenames):\n",
    "\n",
    "    MetadataFilename=folder_path+'/'+file[:-7]+'.mws'\n",
    "\n",
    "    Recordingfile=folder_path+'/'+file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "QR46SEBtZHtv"
   },
   "outputs": [],
   "source": [
    "def notebook_name():\n",
    "\n",
    "\n",
    "\n",
    "    currentNotebook = ipyparams.notebook_name\n",
    "\n",
    "    return currentNotebook\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "W9T2b9p2ZHtv",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def experiment_spike_detection(folder, save_to, identifier1, identifier2, MetadataMaskPath, Analyze,\n",
    "                               detmethod=AP_detection_lofnew,  onlyLed=True,\n",
    "                               E_Stim=False, fs=20000, cutoff=100, highpassth=5, order=9, dead_time=0.5, threshold=4.5, usewelllist=[], \n",
    "                               negative=True,\n",
    "                               positive=True, PipNoise=False, noallwells=False, mindur=3, maxdur=500):\n",
    "\n",
    "    \"\"\"Function to run spike detection on experimental folder and save detectios\"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    folder_path, meta_path, save=Fold_Folders(folder)\n",
    "    \n",
    "    \n",
    "    print(folder_path, meta_path, save, 'new_paths')\n",
    "    \n",
    "    \n",
    "    if len(save_to)<1:\n",
    "        \n",
    "        save_to=save\n",
    "\n",
    "\n",
    "    filenames, experiments=load_raw(folder_path, identifier1, identifier1)\n",
    "    ES_entry={}\n",
    "    LED_entry={}\n",
    "    LabBook={}\n",
    "    stamp={}\n",
    "    \n",
    "    PipArtifacts={}\n",
    "\n",
    "\n",
    "\n",
    "    lof = LocalOutlierFactor(novelty=True, contamination=0.1)\n",
    "\n",
    "    for file in filenames:\n",
    "\n",
    "\n",
    "        MetadataFilename=folder_path+'/'+file[:-7]+'.mws'\n",
    "\n",
    "        Recordingfile=meta_path+'/'+file\n",
    "        recording=McsPy.McsData.RawData(Recordingfile)\n",
    "\n",
    "        tree = etree.parse(MetadataFilename)\n",
    "        ExperimentID=tree.xpath('//ExperimentID/text()')[0]\n",
    "        CompoundID=recording.comment\n",
    "        Ahigh=int(tree.xpath('//FrequencymHz/text()')[0])/1000\n",
    "        Alow=int(tree.xpath('//FrequencymHz/text()')[1])/1000\n",
    "\n",
    "        time=recording.recordings[0].analog_streams[1].timestamp_index[:, 0]\n",
    "        #print(time, 'time')\n",
    "        #print(experiments)\n",
    "        #print(CompoundID)\n",
    "        experiments[ExperimentID][CompoundID].extend(time[:].tolist())\n",
    "\n",
    "    #print(experiments)\n",
    "    #print(filenames)\n",
    "    \n",
    "    filenames, experiments1=load_raw(folder_path, identifier1, identifier2)\n",
    "    \n",
    "    \n",
    "    for file in filenames:\n",
    "\n",
    "        #print(file)\n",
    "\n",
    "        MetadataFilename=meta_path+'/'+file[:-7]+'.mws'\n",
    "\n",
    "        Recordingfile=folder_path+'/'+file\n",
    "\n",
    "\n",
    "        LED_entry[file]=led_info(Recordingfile, MetadataFilename)\n",
    "\n",
    "        \n",
    "        \n",
    "        print(detmethod, 'detmethod')\n",
    "\n",
    "\n",
    "        D=SpikeDetection_EStim(save_to, lof, Recordingfile, MetadataFilename, MetadataMaskPath, experiments, Analyze,\n",
    "                               detmethod, E_Stim, fs, cutoff, highpassth, order, dead_time, threshold, usewelllist, negative,\n",
    "                               positive, PipNoise, noallwells, mindur, maxdur, onlyLed)\n",
    "        \n",
    "       \n",
    "        \n",
    "       \n",
    "        if Analyze=='True':\n",
    "            DF= pd.DataFrame(D[0])\n",
    "            Features=D[5]\n",
    "            DF.to_csv(save_to+'/'+file+'Spikes.csv')\n",
    "            pd.DataFrame(Features).to_csv(save_to+'/'+file+'Spike_Features.csv')\n",
    "            lof=D[2]\n",
    "            save_model(lof, '23.11.23Novelty.pkl')\n",
    "            spike_wvfs=D[6]\n",
    "\n",
    "            np.save(save_to+'/'+'spik_wvfs', spike_wvfs)\n",
    "\n",
    "\n",
    "        ES_entry[file]=D[1]\n",
    "        LED_entry[file]=led_info(Recordingfile, MetadataFilename)\n",
    "        LabBook[file]=D[3]\n",
    "        LabBook['Version']=notebook_name()\n",
    "        LabBook['Sampling rate']=fs\n",
    "        LabBook['Threshold']=[threshold]\n",
    "        LabBook['SignNegPos']=[negative, positive]\n",
    "        LabBook['Highpass']=[highpassth]\n",
    "        LabBook['Method']=[str(detmethod)]\n",
    "        LabBook['AnanlogFhighlow']=[Ahigh, Alow]\n",
    "\n",
    "        stamp[file]=D[4]\n",
    "        \n",
    "        PipArtifacts[file]=D[7]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #save_model(lof, 'Novelty.pkl') #to modify auto\n",
    "\n",
    "    np.save(save_to+'/'+'ES_info', ES_entry)\n",
    "    np.save(save_to+'/'+'LED_info', LED_entry)\n",
    "    np.save(save_to+'/'+'Lab_Book', LabBook)\n",
    "    np.save(save_to+'/'+'stamp', stamp)\n",
    "    np.save(save_to+'/'+'PipArt', PipArtifacts)\n",
    "\n",
    "\n",
    "\n",
    "    return  filenames, stamp, LED_entry, LabBook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "xoWH_j_EZHtv",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "save_to=''\n",
    "\n",
    "folder=r'C:\\Users\\MEA_PC\\Desktop\\AH\\Experiments\\iN\\iN8-2\\Spontan'\n",
    "identifier1='mwd'\n",
    "identifier2='h5'\n",
    "MetadataMaskPath=r'C:\\Users\\MEA_PC\\Desktop\\AH\\Python\\Metadata'\n",
    "ar=np.array([0, 1, 2, 3, 4, 5])\n",
    "\n",
    "Wellsanalyze=np.vstack([ar, ar+6, ar+12, ar+18]).ravel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wellsanalyze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "folder_path,\n",
    "save_to,\n",
    "identifier1, \n",
    "identifier2,\n",
    "MetadataMaskPath,\n",
    "Analyze='True'\n",
    "detmethod=AP_detection_lofnew,\n",
    "onlyLed=True,\n",
    "E_Stim=False, \n",
    "fs=20000,\n",
    "cutoff=100, \n",
    "highpassth=5,\n",
    "\n",
    "order=9,\n",
    "dead_time=0.5, \n",
    "\n",
    "threshold=4.5, \n",
    "usewelllist=[]\n",
    "\n",
    "negative=True,\n",
    "positive=True,\n",
    "PipNoise=False, \n",
    "noallwells=False, \n",
    "mindur=3,\n",
    "maxdur=500):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\MEA_PC\\\\Desktop\\\\AH\\\\Experiments\\\\iN\\\\iN8-2\\\\Spontan'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MEA_PC\\Desktop\\AH\\Experiments\\iN\\iN8-2\\Spontan\\hdf5 C:\\Users\\MEA_PC\\Desktop\\AH\\Experiments\\iN\\iN8-2\\Spontan\\hdf5 C:\\Users\\MEA_PC\\Desktop\\AH\\Experiments\\iN\\iN8-2\\Spontan\\csv new_paths\n",
      "['20220816_No_Group_BP01.1NGN2BP01.1AD2DIV18_001_Control.mws', '20220816_No_Group_BP01.1NGN2BP01.1AD2DIV18_001_Control_mwc.h5', '20220816_No_Group_BP01.1NGN2BP01.1AD2DIV18_001_Control_mwd.h5', '20220824_No_Group__BP01.1NGN2BP01.1AD2DIV26_001_Control.mws', '20220824_No_Group__BP01.1NGN2BP01.1AD2DIV26_001_Control_mwc.h5', '20220824_No_Group__BP01.1NGN2BP01.1AD2DIV26_001_Control_mwd.h5', '20220831_No_Group_BP01.1NGN2BP01.1AD2DIV31_001.mws', '20220831_No_Group_BP01.1NGN2BP01.1AD2DIV31_001_mwc.h5', '20220831_No_Group_BP01.1NGN2BP01.1AD2DIV31_001_mwd.h5', '20220906_No_Group_BP01.1NGN2BP01.1AD2DIV36_001.mws', '20220906_No_Group_BP01.1NGN2BP01.1AD2DIV36_001_mwc.h5', '20220906_No_Group_BP01.1NGN2BP01.1AD2DIV36_001_mwd.h5', '20220913_No_Group_BP01.1NGN2BP01.1AD2DIV42_001.mws', '20220913_No_Group_BP01.1NGN2BP01.1AD2DIV42_001_mwc.h5', '20220913_No_Group_BP01.1NGN2BP01.1AD2DIV42_001_mwd.h5', '20220916_No_Group_BP01.1NGN2BP01.1AD2DIV46_001.mws', '20220916_No_Group_BP01.1NGN2BP01.1AD2DIV46_001_mwc.h5', '20220916_No_Group_BP01.1NGN2BP01.1AD2DIV46_001_mwd.h5', '20220920_No_Group_BP01.1NGN2BP01.1AD2DIV50_001.mws', '20220920_No_Group_BP01.1NGN2BP01.1AD2DIV50_001_mwc.h5', '20220920_No_Group_BP01.1NGN2BP01.1AD2DIV50_001_mwd.h5', '20220927_No_Group_BP01.1NGN2BP01.1AD2DIV57_001.mws', '20220927_No_Group_BP01.1NGN2BP01.1AD2DIV57_001_mwc.h5', '20220927_No_Group_BP01.1NGN2BP01.1AD2DIV57_001_mwd.h5', 'csv'] dirs\n",
      "Recording_0 <HDF5 group \"/Data/Recording_0\" (2 members)>\n",
      "Stream_0 <HDF5 group \"/Data/Recording_0/AnalogStream/Stream_0\" (3 members)>\n",
      "ChannelData <HDF5 dataset \"ChannelData\": shape (288, 4800000), type \"<i4\">\n",
      "ChannelDataTimeStamps <HDF5 dataset \"ChannelDataTimeStamps\": shape (1, 3), type \"<i8\">\n",
      "InfoChannel <HDF5 dataset \"InfoChannel\": shape (288,), type \"|V108\">\n",
      "Stream_1 <HDF5 group \"/Data/Recording_0/AnalogStream/Stream_1\" (3 members)>\n",
      "ChannelData <HDF5 dataset \"ChannelData\": shape (8, 4800000), type \"<i4\">\n",
      "ChannelDataTimeStamps <HDF5 dataset \"ChannelDataTimeStamps\": shape (1, 3), type \"<i8\">\n",
      "InfoChannel <HDF5 dataset \"InfoChannel\": shape (8,), type \"|V108\">\n",
      "Stream_2 <HDF5 group \"/Data/Recording_0/AnalogStream/Stream_2\" (3 members)>\n",
      "ChannelData <HDF5 dataset \"ChannelData\": shape (3, 4800000), type \"<i4\">\n",
      "ChannelDataTimeStamps <HDF5 dataset \"ChannelDataTimeStamps\": shape (1, 3), type \"<i8\">\n",
      "InfoChannel <HDF5 dataset \"InfoChannel\": shape (3,), type \"|V108\">\n",
      "Recording_0 <HDF5 group \"/Data/Recording_0\" (2 members)>\n",
      "Stream_0 <HDF5 group \"/Data/Recording_0/AnalogStream/Stream_0\" (3 members)>\n",
      "ChannelData <HDF5 dataset \"ChannelData\": shape (288, 4800000), type \"<i4\">\n",
      "ChannelDataTimeStamps <HDF5 dataset \"ChannelDataTimeStamps\": shape (1, 3), type \"<i8\">\n",
      "InfoChannel <HDF5 dataset \"InfoChannel\": shape (288,), type \"|V108\">\n",
      "Stream_1 <HDF5 group \"/Data/Recording_0/AnalogStream/Stream_1\" (3 members)>\n",
      "ChannelData <HDF5 dataset \"ChannelData\": shape (8, 4800000), type \"<i4\">\n",
      "ChannelDataTimeStamps <HDF5 dataset \"ChannelDataTimeStamps\": shape (1, 3), type \"<i8\">\n",
      "InfoChannel <HDF5 dataset \"InfoChannel\": shape (8,), type \"|V108\">\n",
      "Stream_2 <HDF5 group \"/Data/Recording_0/AnalogStream/Stream_2\" (3 members)>\n",
      "ChannelData <HDF5 dataset \"ChannelData\": shape (3, 4800000), type \"<i4\">\n",
      "ChannelDataTimeStamps <HDF5 dataset \"ChannelDataTimeStamps\": shape (1, 3), type \"<i8\">\n",
      "InfoChannel <HDF5 dataset \"InfoChannel\": shape (3,), type \"|V108\">\n",
      "Recording_0 <HDF5 group \"/Data/Recording_0\" (2 members)>\n",
      "Stream_0 <HDF5 group \"/Data/Recording_0/AnalogStream/Stream_0\" (3 members)>\n",
      "ChannelData <HDF5 dataset \"ChannelData\": shape (288, 2400000), type \"<i4\">\n",
      "ChannelDataTimeStamps <HDF5 dataset \"ChannelDataTimeStamps\": shape (1, 3), type \"<i8\">\n",
      "InfoChannel <HDF5 dataset \"InfoChannel\": shape (288,), type \"|V108\">\n",
      "Stream_1 <HDF5 group \"/Data/Recording_0/AnalogStream/Stream_1\" (3 members)>\n",
      "ChannelData <HDF5 dataset \"ChannelData\": shape (8, 2400000), type \"<i4\">\n",
      "ChannelDataTimeStamps <HDF5 dataset \"ChannelDataTimeStamps\": shape (1, 3), type \"<i8\">\n",
      "InfoChannel <HDF5 dataset \"InfoChannel\": shape (8,), type \"|V108\">\n",
      "Stream_2 <HDF5 group \"/Data/Recording_0/AnalogStream/Stream_2\" (3 members)>\n",
      "ChannelData <HDF5 dataset \"ChannelData\": shape (3, 2400000), type \"<i4\">\n",
      "ChannelDataTimeStamps <HDF5 dataset \"ChannelDataTimeStamps\": shape (1, 3), type \"<i8\">\n",
      "InfoChannel <HDF5 dataset \"InfoChannel\": shape (3,), type \"|V108\">\n",
      "Recording_0 <HDF5 group \"/Data/Recording_0\" (2 members)>\n",
      "Stream_0 <HDF5 group \"/Data/Recording_0/AnalogStream/Stream_0\" (3 members)>\n",
      "ChannelData <HDF5 dataset \"ChannelData\": shape (288, 4800000), type \"<i4\">\n",
      "ChannelDataTimeStamps <HDF5 dataset \"ChannelDataTimeStamps\": shape (1, 3), type \"<i8\">\n",
      "InfoChannel <HDF5 dataset \"InfoChannel\": shape (288,), type \"|V108\">\n",
      "Stream_1 <HDF5 group \"/Data/Recording_0/AnalogStream/Stream_1\" (3 members)>\n",
      "ChannelData <HDF5 dataset \"ChannelData\": shape (8, 4800000), type \"<i4\">\n",
      "ChannelDataTimeStamps <HDF5 dataset \"ChannelDataTimeStamps\": shape (1, 3), type \"<i8\">\n",
      "InfoChannel <HDF5 dataset \"InfoChannel\": shape (8,), type \"|V108\">\n",
      "Stream_2 <HDF5 group \"/Data/Recording_0/AnalogStream/Stream_2\" (3 members)>\n",
      "ChannelData <HDF5 dataset \"ChannelData\": shape (3, 4800000), type \"<i4\">\n",
      "ChannelDataTimeStamps <HDF5 dataset \"ChannelDataTimeStamps\": shape (1, 3), type \"<i8\">\n",
      "InfoChannel <HDF5 dataset \"InfoChannel\": shape (3,), type \"|V108\">\n",
      "Recording_0 <HDF5 group \"/Data/Recording_0\" (2 members)>\n",
      "Stream_0 <HDF5 group \"/Data/Recording_0/AnalogStream/Stream_0\" (3 members)>\n",
      "ChannelData <HDF5 dataset \"ChannelData\": shape (288, 4800000), type \"<i4\">\n",
      "ChannelDataTimeStamps <HDF5 dataset \"ChannelDataTimeStamps\": shape (1, 3), type \"<i8\">\n",
      "InfoChannel <HDF5 dataset \"InfoChannel\": shape (288,), type \"|V108\">\n",
      "Stream_1 <HDF5 group \"/Data/Recording_0/AnalogStream/Stream_1\" (3 members)>\n",
      "ChannelData <HDF5 dataset \"ChannelData\": shape (8, 4800000), type \"<i4\">\n",
      "ChannelDataTimeStamps <HDF5 dataset \"ChannelDataTimeStamps\": shape (1, 3), type \"<i8\">\n",
      "InfoChannel <HDF5 dataset \"InfoChannel\": shape (8,), type \"|V108\">\n",
      "Stream_2 <HDF5 group \"/Data/Recording_0/AnalogStream/Stream_2\" (3 members)>\n",
      "ChannelData <HDF5 dataset \"ChannelData\": shape (3, 4800000), type \"<i4\">\n",
      "ChannelDataTimeStamps <HDF5 dataset \"ChannelDataTimeStamps\": shape (1, 3), type \"<i8\">\n",
      "InfoChannel <HDF5 dataset \"InfoChannel\": shape (3,), type \"|V108\">\n",
      "Recording_0 <HDF5 group \"/Data/Recording_0\" (2 members)>\n",
      "Stream_0 <HDF5 group \"/Data/Recording_0/AnalogStream/Stream_0\" (3 members)>\n",
      "ChannelData <HDF5 dataset \"ChannelData\": shape (288, 4800000), type \"<i4\">\n",
      "ChannelDataTimeStamps <HDF5 dataset \"ChannelDataTimeStamps\": shape (1, 3), type \"<i8\">\n",
      "InfoChannel <HDF5 dataset \"InfoChannel\": shape (288,), type \"|V108\">\n",
      "Stream_1 <HDF5 group \"/Data/Recording_0/AnalogStream/Stream_1\" (3 members)>\n",
      "ChannelData <HDF5 dataset \"ChannelData\": shape (8, 4800000), type \"<i4\">\n",
      "ChannelDataTimeStamps <HDF5 dataset \"ChannelDataTimeStamps\": shape (1, 3), type \"<i8\">\n",
      "InfoChannel <HDF5 dataset \"InfoChannel\": shape (8,), type \"|V108\">\n",
      "Stream_2 <HDF5 group \"/Data/Recording_0/AnalogStream/Stream_2\" (3 members)>\n",
      "ChannelData <HDF5 dataset \"ChannelData\": shape (3, 4800000), type \"<i4\">\n",
      "ChannelDataTimeStamps <HDF5 dataset \"ChannelDataTimeStamps\": shape (1, 3), type \"<i8\">\n",
      "InfoChannel <HDF5 dataset \"InfoChannel\": shape (3,), type \"|V108\">\n",
      "Recording_0 <HDF5 group \"/Data/Recording_0\" (2 members)>\n",
      "Stream_0 <HDF5 group \"/Data/Recording_0/AnalogStream/Stream_0\" (3 members)>\n",
      "ChannelData <HDF5 dataset \"ChannelData\": shape (288, 4800000), type \"<i4\">\n",
      "ChannelDataTimeStamps <HDF5 dataset \"ChannelDataTimeStamps\": shape (1, 3), type \"<i8\">\n",
      "InfoChannel <HDF5 dataset \"InfoChannel\": shape (288,), type \"|V108\">\n",
      "Stream_1 <HDF5 group \"/Data/Recording_0/AnalogStream/Stream_1\" (3 members)>\n",
      "ChannelData <HDF5 dataset \"ChannelData\": shape (8, 4800000), type \"<i4\">\n",
      "ChannelDataTimeStamps <HDF5 dataset \"ChannelDataTimeStamps\": shape (1, 3), type \"<i8\">\n",
      "InfoChannel <HDF5 dataset \"InfoChannel\": shape (8,), type \"|V108\">\n",
      "Stream_2 <HDF5 group \"/Data/Recording_0/AnalogStream/Stream_2\" (3 members)>\n",
      "ChannelData <HDF5 dataset \"ChannelData\": shape (3, 4800000), type \"<i4\">\n",
      "ChannelDataTimeStamps <HDF5 dataset \"ChannelDataTimeStamps\": shape (1, 3), type \"<i8\">\n",
      "InfoChannel <HDF5 dataset \"InfoChannel\": shape (3,), type \"|V108\">\n",
      "Recording_0 <HDF5 group \"/Data/Recording_0\" (2 members)>\n",
      "Stream_0 <HDF5 group \"/Data/Recording_0/AnalogStream/Stream_0\" (3 members)>\n",
      "ChannelData <HDF5 dataset \"ChannelData\": shape (288, 4800000), type \"<i4\">\n",
      "ChannelDataTimeStamps <HDF5 dataset \"ChannelDataTimeStamps\": shape (1, 3), type \"<i8\">\n",
      "InfoChannel <HDF5 dataset \"InfoChannel\": shape (288,), type \"|V108\">\n",
      "Stream_1 <HDF5 group \"/Data/Recording_0/AnalogStream/Stream_1\" (3 members)>\n",
      "ChannelData <HDF5 dataset \"ChannelData\": shape (8, 4800000), type \"<i4\">\n",
      "ChannelDataTimeStamps <HDF5 dataset \"ChannelDataTimeStamps\": shape (1, 3), type \"<i8\">\n",
      "InfoChannel <HDF5 dataset \"InfoChannel\": shape (8,), type \"|V108\">\n",
      "Stream_2 <HDF5 group \"/Data/Recording_0/AnalogStream/Stream_2\" (3 members)>\n",
      "ChannelData <HDF5 dataset \"ChannelData\": shape (3, 4800000), type \"<i4\">\n",
      "ChannelDataTimeStamps <HDF5 dataset \"ChannelDataTimeStamps\": shape (1, 3), type \"<i8\">\n",
      "InfoChannel <HDF5 dataset \"InfoChannel\": shape (3,), type \"|V108\">\n",
      "['20220816_No_Group_BP01.1NGN2BP01.1AD2DIV18_001_Control.mws', '20220816_No_Group_BP01.1NGN2BP01.1AD2DIV18_001_Control_mwc.h5', '20220816_No_Group_BP01.1NGN2BP01.1AD2DIV18_001_Control_mwd.h5', '20220824_No_Group__BP01.1NGN2BP01.1AD2DIV26_001_Control.mws', '20220824_No_Group__BP01.1NGN2BP01.1AD2DIV26_001_Control_mwc.h5', '20220824_No_Group__BP01.1NGN2BP01.1AD2DIV26_001_Control_mwd.h5', '20220831_No_Group_BP01.1NGN2BP01.1AD2DIV31_001.mws', '20220831_No_Group_BP01.1NGN2BP01.1AD2DIV31_001_mwc.h5', '20220831_No_Group_BP01.1NGN2BP01.1AD2DIV31_001_mwd.h5', '20220906_No_Group_BP01.1NGN2BP01.1AD2DIV36_001.mws', '20220906_No_Group_BP01.1NGN2BP01.1AD2DIV36_001_mwc.h5', '20220906_No_Group_BP01.1NGN2BP01.1AD2DIV36_001_mwd.h5', '20220913_No_Group_BP01.1NGN2BP01.1AD2DIV42_001.mws', '20220913_No_Group_BP01.1NGN2BP01.1AD2DIV42_001_mwc.h5', '20220913_No_Group_BP01.1NGN2BP01.1AD2DIV42_001_mwd.h5', '20220916_No_Group_BP01.1NGN2BP01.1AD2DIV46_001.mws', '20220916_No_Group_BP01.1NGN2BP01.1AD2DIV46_001_mwc.h5', '20220916_No_Group_BP01.1NGN2BP01.1AD2DIV46_001_mwd.h5', '20220920_No_Group_BP01.1NGN2BP01.1AD2DIV50_001.mws', '20220920_No_Group_BP01.1NGN2BP01.1AD2DIV50_001_mwc.h5', '20220920_No_Group_BP01.1NGN2BP01.1AD2DIV50_001_mwd.h5', '20220927_No_Group_BP01.1NGN2BP01.1AD2DIV57_001.mws', '20220927_No_Group_BP01.1NGN2BP01.1AD2DIV57_001_mwc.h5', '20220927_No_Group_BP01.1NGN2BP01.1AD2DIV57_001_mwd.h5', 'csv'] dirs\n",
      "Recording_0 <HDF5 group \"/Data/Recording_0\" (2 members)>\n",
      "Stream_0 <HDF5 group \"/Data/Recording_0/EventStream/Stream_0\" (1 members)>\n",
      "InfoEvent <HDF5 dataset \"InfoEvent\": shape (72,), type \"|V44\">\n",
      "Stream_1 <HDF5 group \"/Data/Recording_0/EventStream/Stream_1\" (11 members)>\n",
      "EventEntity_0 <HDF5 dataset \"EventEntity_0\": shape (5, 1), type \"<i8\">\n",
      "EventEntity_1 <HDF5 dataset \"EventEntity_1\": shape (5, 1), type \"<i8\">\n",
      "EventEntity_14 <HDF5 dataset \"EventEntity_14\": shape (5, 1), type \"<i8\">\n",
      "EventEntity_15 <HDF5 dataset \"EventEntity_15\": shape (5, 1), type \"<i8\">\n",
      "EventEntity_16 <HDF5 dataset \"EventEntity_16\": shape (5, 1), type \"<i8\">\n",
      "EventEntity_17 <HDF5 dataset \"EventEntity_17\": shape (5, 1), type \"<i8\">\n",
      "EventEntity_6 <HDF5 dataset \"EventEntity_6\": shape (5, 1), type \"<i8\">\n",
      "EventEntity_7 <HDF5 dataset \"EventEntity_7\": shape (5, 1), type \"<i8\">\n",
      "EventEntity_8 <HDF5 dataset \"EventEntity_8\": shape (5, 1), type \"<i8\">\n",
      "EventEntity_9 <HDF5 dataset \"EventEntity_9\": shape (5, 1), type \"<i8\">\n",
      "InfoEvent <HDF5 dataset \"InfoEvent\": shape (28,), type \"|V44\">\n",
      "Stream_2 <HDF5 group \"/Data/Recording_0/EventStream/Stream_2\" (3 members)>\n",
      "EventEntity_0 <HDF5 dataset \"EventEntity_0\": shape (5, 1), type \"<i8\">\n",
      "EventEntity_1 <HDF5 dataset \"EventEntity_1\": shape (5, 1), type \"<i8\">\n",
      "InfoEvent <HDF5 dataset \"InfoEvent\": shape (2,), type \"|V44\">\n",
      "<function AP_detection_lofnew at 0x0000024E2488BE50> detmethod\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording_0 <HDF5 group \"/Data/Recording_0\" (2 members)>\n",
      "Stream_0 <HDF5 group \"/Data/Recording_0/AnalogStream/Stream_0\" (3 members)>\n",
      "ChannelData <HDF5 dataset \"ChannelData\": shape (288, 4800000), type \"<i4\">\n",
      "ChannelDataTimeStamps <HDF5 dataset \"ChannelDataTimeStamps\": shape (1, 3), type \"<i8\">\n",
      "InfoChannel <HDF5 dataset \"InfoChannel\": shape (288,), type \"|V108\">\n",
      "Stream_1 <HDF5 group \"/Data/Recording_0/AnalogStream/Stream_1\" (3 members)>\n",
      "ChannelData <HDF5 dataset \"ChannelData\": shape (8, 4800000), type \"<i4\">\n",
      "ChannelDataTimeStamps <HDF5 dataset \"ChannelDataTimeStamps\": shape (1, 3), type \"<i8\">\n",
      "InfoChannel <HDF5 dataset \"InfoChannel\": shape (8,), type \"|V108\">\n",
      "Stream_2 <HDF5 group \"/Data/Recording_0/AnalogStream/Stream_2\" (3 members)>\n",
      "ChannelData <HDF5 dataset \"ChannelData\": shape (3, 4800000), type \"<i4\">\n",
      "ChannelDataTimeStamps <HDF5 dataset \"ChannelDataTimeStamps\": shape (1, 3), type \"<i8\">\n",
      "InfoChannel <HDF5 dataset \"InfoChannel\": shape (3,), type \"|V108\">\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function RawData.__del__ at 0x0000024E1831FE50>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\MEA_PC\\anaconda3\\lib\\site-packages\\McsPy\\McsData.py\", line 68, in __del__\n",
      "    if self.h5_file:\n",
      "AttributeError: 'RawData' object has no attribute 'h5_file'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-96-5649deb13ecf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexperiment_spike_detection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_to\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midentifier1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midentifier2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMetadataMaskPath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'True'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mE_Stim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcutoff\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdead_time\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0musewelllist\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mWellsanalyze\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpositive\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPipNoise\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnoallwells\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mmindur\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxdur\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-84-db8c7cd32e2d>\u001b[0m in \u001b[0;36mexperiment_spike_detection\u001b[1;34m(folder, save_to, identifier1, identifier2, MetadataMaskPath, Analyze, detmethod, onlyLed, E_Stim, fs, cutoff, highpassth, order, dead_time, threshold, usewelllist, negative, positive, PipNoise, noallwells, mindur, maxdur)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m         D=SpikeDetection_EStim(save_to, lof, Recordingfile, MetadataFilename, MetadataMaskPath, experiments, Analyze,\n\u001b[0m\u001b[0;32m     78\u001b[0m                                \u001b[0mdetmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mE_Stim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhighpassth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdead_time\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0musewelllist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m                                positive, PipNoise, noallwells, mindur, maxdur, onlyLed)\n",
      "\u001b[1;32m<ipython-input-70-2c442e7f4d9e>\u001b[0m in \u001b[0;36mSpikeDetection_EStim\u001b[1;34m(to, lof, Recordingfilename, MetadataFilename, MetadataMaskPath, experiments, Analyze, detmethod, E_Stim, fs, cutoff, highpassth, order, dead_time, threshold, usewelllist, negative, positive, PipNoise, noallwells, mindur, maxdur, onlyLed)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 251\u001b[1;33m                     \u001b[0mv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrecording\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecordings\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manalog_streams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchannel_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdose_start\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mdose_stop\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    252\u001b[0m                     \u001b[1;31m#fv=butter_bandpass_filter(v, 10, 3500,  20000, order=3)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m                     \u001b[0mfv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhighpass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhighpassth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\h5py\\_hl\\dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    574\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    575\u001b[0m         \u001b[1;31m# Patch up the output for NumPy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 576\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    577\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m     \u001b[1;31m# Single-field recarray convention\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sp=experiment_spike_detection(folder, save_to, identifier1, identifier2, MetadataMaskPath, 'True', E_Stim=False, fs=20000, cutoff=100, order=9, dead_time=0.5, threshold=5, usewelllist=Wellsanalyze, negative=True, positive=True, PipNoise=False, noallwells=False,  mindur=3, maxdur=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hvB-E4IuZHty"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tGj4QzjuZHtz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-IvuhOoeZHtz"
   },
   "outputs": [],
   "source": [
    "####example here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "tshBwsmJZHt0"
   },
   "outputs": [],
   "source": [
    "MetadataMaskPath=r'C:\\Users\\MEA_PC\\Desktop\\AH\\Python\\Metadata'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "SLrWwb8UZHt0"
   },
   "outputs": [],
   "source": [
    "data_path=r'C:\\Users\\MEA_PC\\Desktop\\Katrin\\SN2_olddip16drug\\hdf5\\20231215_veh_SN2_oldDIP16caps10_001_Control_mwd.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "zqAgfFe_ZHt0"
   },
   "outputs": [],
   "source": [
    "R=McsPy.McsData.RawData(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function RawData.__del__ at 0x0000017FBFA34940>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\MEA_PC\\anaconda3\\lib\\site-packages\\McsPy\\McsData.py\", line 68, in __del__\n",
      "    if self.h5_file:\n",
      "AttributeError: 'RawData' object has no attribute 'h5_file'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x17fcd2518e0>]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(R.recordings[0].analog_streams[0].channel_data[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Of0vVANHZHt0",
    "outputId": "c036b81e-45dd-48d1-aca7-0fcd8b16bc09"
   },
   "outputs": [],
   "source": [
    "vs=R.recordings[0].analog_streams[0].channel_data[:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "__G8dmthZHt1"
   },
   "outputs": [],
   "source": [
    "WellID=np.load(MetadataMaskPath+'/'+'WellID.csv.npy')\n",
    "Welllabel=np.load(MetadataMaskPath+'/'+'WellLabel.csv.npy')\n",
    "Channellabel=np.load(MetadataMaskPath+'/'+'ChannelLabel.csv.npy')\n",
    "ChannelID=list(R.recordings[0].analog_streams[0].channel_infos.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vzTo8FkwZHt1"
   },
   "outputs": [],
   "source": [
    "last=ChannelID[29]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9G1LaS1rZHt3"
   },
   "outputs": [],
   "source": [
    "v=vs[29, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V8MjDg8oZHt3"
   },
   "outputs": [],
   "source": [
    "lof=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h2aiu1BLZHt4",
    "outputId": "2a36e55e-76af-4800-a91e-76bbf2702af6"
   },
   "outputs": [],
   "source": [
    "chxor2=AP_detection_lofnew(data_path, lof, v, 0.5, 5, True, True, 20000, 3, False, start=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.hist(chxor2[2], bins=100, alpha=0.5)\n",
    "plt.xlabel('ADCvalues')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Amplitude histogram (median-peak)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "LO5W19gHZHuR",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def test_main(Recordingfilename, MetadataFilename, MetadataMaskPath):\n",
    "\n",
    "\n",
    "    \"\"\"Function to detect spike and write spike detections in a table for a single recording file\"\"\"\n",
    "\n",
    "    D={'Timestamp [µs]':[], 'Peak Amplitudes':[], 'Duration':[], 'Channel ID':[], 'Well ID':[], 'Well Label':[],\n",
    "       'Channel Label':[], 'Experiment':[], 'Dose Label':[], 'Compound ID':[], 'ES_condition':[]}\n",
    "    recording=McsPy.McsData.RawData(Recordingfilename)\n",
    "    #print(recording.recordings[0].event_streams.keys())\n",
    "\n",
    "    tree = etree.parse(MetadataFilename)\n",
    "    lab=lab_book(tree)\n",
    "    WellID=np.load(MetadataMaskPath+'/'+'WellID.csv.npy')\n",
    "    Welllabel=np.load(MetadataMaskPath+'/'+'WellLabel.csv.npy')\n",
    "    Channellabel=np.load(MetadataMaskPath+'/'+'ChannelLabel.csv.npy')\n",
    "    ExperimentID=tree.xpath('//ExperimentID/text()')[0]\n",
    "\n",
    "\n",
    "    ChannelID=list(recording.recordings[0].analog_streams[0].channel_infos.keys())\n",
    "    #print(list(recording.recordings[0].analog_streams[0].channel_infos.keys()))\n",
    "\n",
    "    ChannelID=[int(i) for i in ChannelID]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for i in ChannelID:\n",
    "\n",
    "\n",
    "            index=int(recording.recordings[0].analog_streams[0].channel_infos[i].row_index)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    Compounds=tree.xpath('//CompoundID/text()')\n",
    "    #print(Compounds)\n",
    "    Compound=recording.comment\n",
    "\n",
    "    Labels=Dilutions(tree, Compound)\n",
    "    starts=recording.recordings[0].analog_streams[1].timestamp_index[:, 1].astype('int64')\n",
    "    stops=recording.recordings[0].analog_streams[1].timestamp_index[:, 2].astype('int64')\n",
    "    esim=recording.recordings[0].analog_streams[1].timestamp_index[:, 0].astype('int64')\n",
    "\n",
    "    #print(starts, stops, esim, 'start, stops, esim')\n",
    "\n",
    "\n",
    "\n",
    "    for j in range(len(starts)):\n",
    "\n",
    "        dose_label=Labels[j]\n",
    "        dose_start=int(starts[j])\n",
    "        dose_stop=int(stops[j])\n",
    "        #print(dose_start, dose_stop, dose_label)\n",
    "\n",
    "    ES_Electrodes=tree.xpath('//ChannelID/text()')\n",
    "    info=led_info(Recordingfilename, MetadataFilename)\n",
    "\n",
    "\n",
    "    return lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Xv_k8AfZHuR",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
